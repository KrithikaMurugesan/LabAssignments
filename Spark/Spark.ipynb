{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Analytics - Exercise 9(Krithika Murugesan -277537)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Apache spark basics : Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given lists are stored as RDD and the join operations are used to perform the right outer join, where all elements from first table are taken with matching entries from second table, while full outer join is intersection of all elements. The frequency of character s in both the tables is done using map reduce and aggregate function after joining the two tables together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#Given lists\n",
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "\n",
    "#To RDD\n",
    "rddA = sc.parallelize(a)\n",
    "rddB = sc.parallelize(b)\n",
    "\n",
    "A = rddA.map(lambda x: Row(name = x))\n",
    "B = rddB.map(lambda x: Row(name = x))\n",
    "\n",
    "schemaA = sqlContext.createDataFrame(A)\n",
    "schemaB = sqlContext.createDataFrame(B)\n",
    "\n",
    "print(type(schemaA),type(schemaB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|operation|\n",
      "|   lambda|\n",
      "|partition|\n",
      "| parallel|\n",
      "|    scala|\n",
      "|   apache|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Right outer join\n",
    "schemaA.join(schemaB, \"name\" ,\"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|operation|\n",
      "|   lambda|\n",
      "|  context|\n",
      "|partition|\n",
      "|   create|\n",
      "|      rdd|\n",
      "| parallel|\n",
      "|    scala|\n",
      "|   apache|\n",
      "|    spark|\n",
      "|    class|\n",
      "|   python|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Full outer join\n",
    "joined = schemaA.join(schemaB, \"name\" ,\"full_outer\")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|operation|\n",
      "|   lambda|\n",
      "|  context|\n",
      "|partition|\n",
      "|   create|\n",
      "|      rdd|\n",
      "| parallel|\n",
      "|    scala|\n",
      "|   apache|\n",
      "|    spark|\n",
      "|    class|\n",
      "|   python|\n",
      "+---------+\n",
      "\n",
      "No: of 's' in Schema A and B  4\n"
     ]
    }
   ],
   "source": [
    "#Map reduce : frequncy of s,where map counts s in seperate parts and reduce sums it\n",
    "joined.show()\n",
    "count = joined.rdd.map(lambda x: sum([each.count('s') for each in x]))\n",
    "                                                .reduce(lambda x, y: x + y)\n",
    "print(\"No: of 's' in Schema A and B \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No: of 's' in Schema A and B  4\n"
     ]
    }
   ],
   "source": [
    "#Using aggregate function, count \"s\"\n",
    "import pyspark.sql.functions as F\n",
    "count = joined.rdd.aggregate(0, lambda i, x: i + x[0].count('s'), lambda i, j: i+j)\n",
    "print(\"No: of 's' in Schema A and B \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part b) Basic Operations on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'s_id': 1, 'first_name': 'Alan', 'last_name': 'Joe', 'dob': 'October 14, 1983', 'course': 'Humanities and Art', 'points': 10}, {'s_id': 2, 'first_name': 'Martin', 'last_name': 'Genberg', 'dob': 'September 26, 1980', 'course': 'Computer Science', 'points': 17}, {'s_id': 3, 'first_name': 'Athur', 'last_name': 'Watson', 'dob': 'June 12, 1982', 'course': 'Graphic Design', 'points': 16}, {'s_id': 4, 'first_name': 'Anabelle', 'last_name': 'Sanberg', 'dob': 'April 5, 1987', 'course': 'Graphic Design', 'points': 12}, {'s_id': 5, 'first_name': 'Kira', 'last_name': 'Schommer', 'dob': 'November 1, 1978', 'course': 'Psychology', 'points': 11}, {'s_id': 6, 'first_name': 'Christian', 'last_name': 'Kiriam', 'dob': '17 February 1981', 'course': 'Business', 'points': 10}, {'s_id': 7, 'first_name': 'Barbara', 'last_name': 'Ballard', 'dob': '1 January 1984', 'course': 'Machine Learning', 'points': 14}, {'s_id': 8, 'first_name': 'John', 'last_name': None, 'dob': 'January 13, 1978', 'course': 'Deep Learning', 'points': 10}, {'s_id': 9, 'first_name': 'Marcus', 'last_name': 'Carson', 'dob': '26 December 1989', 'course': 'Machine Learning', 'points': 15}, {'s_id': 10, 'first_name': 'Marta', 'last_name': 'Brooks', 'dob': '30 December 1987', 'course': 'Physics', 'points': 11}, {'s_id': 11, 'first_name': 'Holly', 'last_name': 'Schwartz', 'dob': 'June 12, 1975', 'course': 'Data Analytics', 'points': 12}, {'s_id': 12, 'first_name': 'April', 'last_name': 'Black', 'dob': 'July 2, 1985', 'course': 'Computer Science', 'points': None}, {'s_id': 13, 'first_name': 'Irene', 'last_name': 'Bradley', 'dob': 'July 22, 1980', 'course': 'Computer Science', 'points': 13}, {'s_id': 14, 'first_name': 'Mark', 'last_name': 'Weber', 'dob': '7 February 1986', 'course': 'Psychology', 'points': 12}, {'s_id': 15, 'first_name': 'Rosie', 'last_name': 'Norman', 'dob': 'May 18, 1987', 'course': 'Informatics', 'points': 9}, {'s_id': 16, 'first_name': 'Martin', 'last_name': 'Steele', 'dob': 'August 10, 1984', 'course': 'Business', 'points': 7}, {'s_id': 17, 'first_name': 'Colin', 'last_name': 'Martinez', 'dob': '16 December 1990', 'course': 'Machine Learning', 'points': 9}, {'s_id': 18, 'first_name': 'Bridget', 'last_name': 'Twain', 'dob': None, 'course': 'Data Analytics', 'points': 6}, {'s_id': 19, 'first_name': 'Darlene', 'last_name': 'Mills', 'dob': '7 March 1980', 'course': 'Business', 'points': 19}, {'s_id': 20, 'first_name': 'Zachary', 'last_name': None, 'dob': 'June 2, 1985', 'course': 'Data Analytics', 'points': 10}]\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading the json file into RDD\n",
    "import json\n",
    "\n",
    "stud = []\n",
    "for line in open('Student.json', 'r'):\n",
    "    stud.append(json.loads(line))\n",
    "print(stud)\n",
    "\n",
    "temp=[json.dumps(stud)]\n",
    "jsonRDD = sc.parallelize(temp)\n",
    "df = sqlContext.read.json(jsonRDD)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Replacing the missing values in the points table by the avergae value, first the average is computed and the Nana are replaced\n",
    "\n",
    "2. Replacing the missing values in other columns as well using fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       __|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|   August 15, 1991|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       __|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compute average\n",
    "from pyspark.sql.functions import avg\n",
    "m = (df.select(avg(\"Points\"))).toPandas()\n",
    "m = float(m.iloc[0])\n",
    "\n",
    "#Replacing missing values with average\n",
    "df1 = df.fillna(m)\n",
    "\n",
    "#Replacing Nan with unknown and --\n",
    "df1 = df1.fillna({'dob':\"August 15, 1991\",'last_name':\"__\"})\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Changing the date format in dob column since each row is of different format, using date parser to get the date parts and convert it to required format as new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|            course|               dob|first_name|last_name|points|s_id|   new_col|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984|\n",
      "|     Deep Learning|  January 13, 1978|      John|       __|    10|   8|13-01-1978|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990|\n",
      "|    Data Analytics|   August 15, 1991|   Bridget|    Twain|     6|  18|15-08-1991|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       __|    10|  20|02-06-1985|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DAtetime packages\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "from pyspark.sql.types import TimestampType,DateType\n",
    "from pyspark.sql.functions import UserDefinedFunction,col,date_format\n",
    "\n",
    "#Extract the date information using parser\n",
    "udf = UserDefinedFunction(lambda x:parser.parse(x), TimestampType())\n",
    "df2 = df1.withColumn(\"New\",udf(df1.dob))\n",
    "\n",
    "#Convert the time stamp to required format,filling unknown with random value \n",
    "#to avoid errors...It will be replaced at the end\n",
    "func =  UserDefinedFunction(lambda x: datetime.datetime\n",
    "                            .strptime(str(x), '%Y-%m-%d %H:%M:%S'), TimestampType())\n",
    "df = df2.withColumn('new_col', date_format(func(col('New')), 'dd-MM-yyy'))        \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Insert age, a coulmn with value current date-dob is added to the RDD to get the currrent age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|            course|               dob|first_name|last_name|points|s_id|   new_col|Age|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983| 35|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980| 38|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982| 36|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987| 31|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978| 40|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981| 37|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984| 34|\n",
      "|     Deep Learning|  January 13, 1978|      John|       __|    10|   8|13-01-1978| 40|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989| 29|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987| 31|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975| 43|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985| 33|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980| 38|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986| 32|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987| 31|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984| 34|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990| 28|\n",
      "|    Data Analytics|   August 15, 1991|   Bridget|    Twain|     6|  18|15-08-1991| 27|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980| 38|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       __|    10|  20|02-06-1985| 33|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get current year\n",
    "from pyspark.sql.functions import year\n",
    "i = 2018\n",
    "\n",
    "df = df.withColumn(\"Age\",i-year(col(\"new_col\")))\n",
    "\n",
    "targetDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|            course|               dob|first_name|last_name|points|s_id|   new_col|Age|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983| 35|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980| 38|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982| 36|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987| 31|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978| 40|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981| 37|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984| 34|\n",
      "|     Deep Learning|  January 13, 1978|      John|       __|    10|   8|13-01-1978| 40|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989| 29|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987| 31|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975| 43|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985| 33|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980| 38|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986| 32|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987| 31|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984| 34|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990| 28|\n",
      "|    Data Analytics|   August 15, 1991|   Bridget|    Twain|     6|  18|15-08-1991| 27|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980| 38|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       __|    10|  20|02-06-1985| 33|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replacing the random values, the final output\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "targetDf = df.withColumn(\"new_col\",when(df[\"dob\"] == 'Feb 28, 2018', \"Unknown\")\n",
    "                         .otherwise(df['new_col']))\n",
    "targetDf = targetDf.withColumn(\"dob\",when(df[\"dob\"] == 'Feb 28, 2018', \"Unknown\")\n",
    "                               .otherwise(df['dob']))\n",
    "\n",
    "targetDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Updating points to 20 if the score is one stddev greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|            course|               dob|first_name|last_name|points|s_id|   new_col|Age|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    20|   1|14-10-1983| 35|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    20|   2|26-09-1980| 38|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    20|   3|12-06-1982| 36|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    20|   4|05-04-1987| 31|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    20|   5|01-11-1978| 40|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    20|   6|17-02-1981| 37|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    20|   7|01-01-1984| 34|\n",
      "|     Deep Learning|  January 13, 1978|      John|       __|    20|   8|13-01-1978| 40|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    20|   9|26-12-1989| 29|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    20|  10|30-12-1987| 31|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    20|  11|12-06-1975| 43|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    20|  12|02-07-1985| 33|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    20|  13|22-07-1980| 38|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    20|  14|07-02-1986| 32|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|    20|  15|18-05-1987| 31|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|    20|  16|10-08-1984| 34|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|    20|  17|16-12-1990| 28|\n",
      "|    Data Analytics|   August 15, 1991|   Bridget|    Twain|    20|  18|15-08-1991| 27|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    20|  19|07-03-1980| 38|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       __|    20|  20|02-06-1985| 33|\n",
      "+------------------+------------------+----------+---------+------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "#Computing stddev\n",
    "df_stats = targetDf.select(_stddev(col('points')).alias('std')).collect()\n",
    "std = df_stats[0]['std']\n",
    "#print(std)\n",
    "\n",
    "#Conditionally checking if points > std, then changing it to 20 else leaving it as it is\n",
    "targetDf = targetDf.withColumn(\"points\",when(targetDf[\"points\"] > std, 20)\n",
    "                               .otherwise(targetDf['dob']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Histogram of previous result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['20', '20'], [20])\n"
     ]
    }
   ],
   "source": [
    "#generating values using rdd functions\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "histogram = targetDf.select('points').rdd.flatMap(lambda x: x).histogram(5)\n",
    "\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  0.,  0.,  0., 20.,  0.,  0.,  0.,  0.]),\n",
       " array([19.5, 19.6, 19.7, 19.8, 19.9, 20. , 20.1, 20.2, 20.3, 20.4, 20.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEiNJREFUeJzt3X+QXWddx/H3h6aolGpTu639FYJa\nO1bGBtwJYBXB2tBGhqKitKMQoEz8ATPgqGP9MVThn/oDHLUOndhGCoMVBQp1CLSZykxhBkq3NZSU\nFFJqtUtiEwy2IDoY/frHPRkv27vZyz13dxOe92vmzD3nOc9zzvN00889++w596aqkCS140mr3QFJ\n0soy+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNWbPaHRjltNNOq/Xr1692NyTp\nuHHPPfd8sapmxql7TAb/+vXrmZubW+1uSNJxI8k/j1vXqR5JaozBL0mNMfglqTEGvyQ1xuCXpMYs\nGfxJzk3ykSR7ktyf5PVd+alJdibZ272uXaT9lq7O3iRbpj0ASdI3Zpwr/sPAr1XV9wPPAV6b5ALg\nauCOqjoPuKPb/jpJTgWuAZ4NbASuWewNQpK0MpYM/qraX1X3dutfBvYAZwOXAzd11W4CXjKi+QuB\nnVV1qKq+BOwELp1GxyVJk/mG5viTrAeeCdwFnFFV+2Hw5gCcPqLJ2cAjQ9vzXZkkaZWM/eRukqcC\n7wXeUFWPJxmr2Yiykd/unmQrsBVg3bp143ZLWlHrr/7gqpz34Wt/clXOq29OY13xJzmRQei/q6re\n1xU/muTMbv+ZwIERTeeBc4e2zwH2jTpHVW2rqtmqmp2ZGevjJiRJExjnrp4ANwJ7quqtQ7tuBY7c\npbMF+MCI5rcBm5Ks7f6ou6krkyStknGu+C8CXg78eJJd3bIZuBa4JMle4JJumySzSW4AqKpDwJuB\nu7vlTV2ZJGmVLDnHX1UfY/RcPcDFI+rPAa8Z2t4ObJ+0g5Kk6fLJXUlqjMEvSY0x+CWpMQa/JDXG\n4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+\nSWrMkt/AlWQ78CLgQFU9oyt7N3B+V+UU4N+rasOItg8DXwb+BzhcVbNT6rckaUJLBj/wduA64B1H\nCqrqZUfWk7wFeOwo7V9QVV+ctIOSpOka5zt370yyftS+JAF+Dvjx6XZLkrRc+s7x/yjwaFXtXWR/\nAbcnuSfJ1p7nkiRNwThTPUdzJXDzUfZfVFX7kpwO7EzyQFXdOapi98awFWDdunU9uyVJWszEV/xJ\n1gA/Dbx7sTpVta97PQDcAmw8St1tVTVbVbMzMzOTdkuStIQ+Uz0/ATxQVfOjdiY5KcnJR9aBTcDu\nHueTJE3BksGf5Gbg48D5SeaTXNXtuoIF0zxJzkqyo9s8A/hYkk8BnwQ+WFUfnl7XJUmTGOeunisX\nKX/liLJ9wOZu/SHgwp79kyRNmU/uSlJjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj\n8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmPG+erF7UkOJNk9VPZ7Sb6Q\nZFe3bF6k7aVJPpvkwSRXT7PjkqTJjHPF/3bg0hHlf1JVG7plx8KdSU4A/gK4DLgAuDLJBX06K0nq\nb8ngr6o7gUMTHHsj8GBVPVRVXwP+Brh8guNIkqaozxz/65Lc100FrR2x/2zgkaHt+a5spCRbk8wl\nmTt48GCPbkmSjmbS4H8b8D3ABmA/8JYRdTKirBY7YFVtq6rZqpqdmZmZsFuSpKVMFPxV9WhV/U9V\n/S/wlwymdRaaB84d2j4H2DfJ+SRJ0zNR8Cc5c2jzp4DdI6rdDZyX5OlJngxcAdw6yfkkSdOzZqkK\nSW4Gng+clmQeuAZ4fpINDKZuHgZ+sat7FnBDVW2uqsNJXgfcBpwAbK+q+5dlFJKksS0Z/FV15Yji\nGxepuw/YPLS9A3jCrZ6SpNXjk7uS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8\nktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUmCWDP8n2JAeS7B4q+6MkDyS5\nL8ktSU5ZpO3DST6dZFeSuWl2XJI0mXGu+N8OXLqgbCfwjKr6QeBzwG8dpf0LqmpDVc1O1kVJ0jQt\nGfxVdSdwaEHZ7VV1uNv8BHDOMvRNkrQMpjHH/2rgQ4vsK+D2JPck2Xq0gyTZmmQuydzBgwen0C1J\n0ii9gj/J7wCHgXctUuWiqnoWcBnw2iTPW+xYVbWtqmaranZmZqZPtyRJRzFx8CfZArwI+PmqqlF1\nqmpf93oAuAXYOOn5JEnTMVHwJ7kU+E3gxVX11UXqnJTk5CPrwCZg96i6kqSVM87tnDcDHwfOTzKf\n5CrgOuBkYGd3q+b1Xd2zkuzomp4BfCzJp4BPAh+sqg8vyygkSWNbs1SFqrpyRPGNi9TdB2zu1h8C\nLuzVO0nS1PnkriQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS\n1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDVmrOBPsj3JgSS7h8pOTbIzyd7ude0ibbd0dfZ2\nX9AuSVpF417xvx24dEHZ1cAdVXUecEe3/XWSnApcAzwb2Ahcs9gbhCRpZYwV/FV1J3BoQfHlwE3d\n+k3AS0Y0fSGws6oOVdWXgJ088Q1EkrSC+szxn1FV+wG619NH1DkbeGRoe74re4IkW5PMJZk7ePBg\nj25Jko5muf+4mxFlNapiVW2rqtmqmp2ZmVnmbklSu/oE/6NJzgToXg+MqDMPnDu0fQ6wr8c5JUk9\n9Qn+W4Ejd+lsAT4wos5twKYka7s/6m7qyiRJq2Tc2zlvBj4OnJ9kPslVwLXAJUn2Apd02ySZTXID\nQFUdAt4M3N0tb+rKJEmrZM04larqykV2XTyi7hzwmqHt7cD2iXonSZo6n9yVpMYY/JLUGINfkhpj\n8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/\nJDVm4uBPcn6SXUPL40nesKDO85M8NlTnjf27LEnqY6xv4Bqlqj4LbABIcgLwBeCWEVU/WlUvmvQ8\nkqTpmtZUz8XA56vqn6d0PEnSMplW8F8B3LzIvucm+VSSDyX5gSmdT5I0od7Bn+TJwIuBvxux+17g\naVV1IfDnwPuPcpytSeaSzB08eLBvtyRJi5jGFf9lwL1V9ejCHVX1eFV9pVvfAZyY5LRRB6mqbVU1\nW1WzMzMzU+iWJGmUaQT/lSwyzZPku5KkW9/Yne/fpnBOSdKEJr6rByDJU4BLgF8cKvslgKq6Hngp\n8MtJDgP/CVxRVdXnnJKkfnoFf1V9FfjOBWXXD61fB1zX5xySpOnyyV1JaozBL0mNMfglqTEGvyQ1\nxuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMM\nfklqTO/gT/Jwkk8n2ZVkbsT+JPmzJA8muS/Js/qeU5I0uV5fvTjkBVX1xUX2XQac1y3PBt7WvUqS\nVsFKTPVcDryjBj4BnJLkzBU4ryRphGkEfwG3J7knydYR+88GHhnanu/KJEmrYBpTPRdV1b4kpwM7\nkzxQVXcO7c+INrWwoHvT2Aqwbt26KXRLkjRK7yv+qtrXvR4AbgE2LqgyD5w7tH0OsG/EcbZV1WxV\nzc7MzPTtliRpEb2CP8lJSU4+sg5sAnYvqHYr8Iru7p7nAI9V1f4+55UkTa7vVM8ZwC1Jjhzrr6vq\nw0l+CaCqrgd2AJuBB4GvAq/qeU5JUg+9gr+qHgIuHFF+/dB6Aa/tcx5J0vT45K4kNcbgl6TGGPyS\n1BiDX5IaY/BLUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mN\nMfglqTEGvyQ1ZuLgT3Juko8k2ZPk/iSvH1Hn+UkeS7KrW97Yr7uSpL76fPXiYeDXqure7gvX70my\ns6o+s6DeR6vqRT3OI0maoomv+Ktqf1Xd261/GdgDnD2tjkmSlsdU5viTrAeeCdw1Yvdzk3wqyYeS\n/MA0zidJmlyfqR4AkjwVeC/whqp6fMHue4GnVdVXkmwG3g+ct8hxtgJbAdatW9e3W5KkRfS64k9y\nIoPQf1dVvW/h/qp6vKq+0q3vAE5MctqoY1XVtqqararZmZmZPt2SJB1Fn7t6AtwI7Kmqty5S57u6\neiTZ2J3v3yY9pySpvz5TPRcBLwc+nWRXV/bbwDqAqroeeCnwy0kOA/8JXFFV1eOckqSeJg7+qvoY\nkCXqXAdcN+k5JEnT55O7ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINf\nkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1Ji+X7Z+aZLPJnkwydUj9n9Lknd3++9K\nsr7P+SRJ/fX5svUTgL8ALgMuAK5McsGCalcBX6qq7wX+BPiDSc8nSZqOPlf8G4EHq+qhqvoa8DfA\n5QvqXA7c1K2/B7g4yVG/p1eStLz6BP/ZwCND2/Nd2cg6VXUYeAz4zh7nlCT1tKZH21FX7jVBnUHF\nZCuwtdv8SpLP9ujbajgN+OJqd2KFOeYVktWdJPXnfHx42rgV+wT/PHDu0PY5wL5F6swnWQN8B3Bo\n1MGqahuwrUd/VlWSuaqaXe1+rCTH3AbH/M2nz1TP3cB5SZ6e5MnAFcCtC+rcCmzp1l8K/ENVjbzi\nlyStjImv+KvqcJLXAbcBJwDbq+r+JG8C5qrqVuBG4J1JHmRwpX/FNDotSZpcn6keqmoHsGNB2RuH\n1v8L+Nk+5ziOHLfTVD045jY45m8yceZFktriRzZIUmMM/iUk2Z7kQJLdQ2UXJvl4kk8n+fsk375I\n21OSvCfJA0n2JHnuyvV8cj3H/KtJ7k+yO8nNSb515Xo+uSTnJvlI93O6P8nru/JTk+xMsrd7XbtI\n+y1dnb1JtoyqcyzpM94kG7p/C/cnuS/Jy1Z+BN+4vj/jru63J/lCkutWrufLoKpcjrIAzwOeBewe\nKrsb+LFu/dXAmxdpexPwmm79ycApqz2e5Rwzgwf2/gn4tm77b4FXrvZ4xhzzmcCzuvWTgc8x+CiS\nPwSu7sqvBv5gRNtTgYe617Xd+trVHtMyjvf7gPO69bOA/cfDv+0+Yx46xp8Cfw1ct9rj6bN4xb+E\nqrqTJz57cD5wZ7e+E/iZhe26K+LnMbiziar6WlX9+zJ2dWomHXNnDfBt3XMbT+GJz3Yck6pqf1Xd\n261/GdjD4I1s+GNHbgJeMqL5C4GdVXWoqr7E4L/Ppcvf68n1GW9Vfa6q9nbr+4ADwMxK9LuPnj9j\nkvwQcAZw+/L3dnkZ/JPZDby4W/9Zvv5BtiO+GzgI/FWSf0xyQ5KTVqqDy2DJMVfVF4A/Bv6FwVXg\nY1V13P1P0n2K7DOBu4Azqmo/DIIDOH1Ek3E+vuSYNcF4h9tuZPDb7OeXt5fT9Y2OOcmTgLcAv7Fy\nvVw+Bv9kXg28Nsk9DH5l/NqIOmsYTJe8raqeCfwHg18jj1dLjrmbG70ceDqDKYCTkvzCivaypyRP\nBd4LvKGqHh+32Yiy4+J2uQnHe6TtmcA7gVdV1f8uR/+Ww4Rj/hVgR1U9smTN44DBP4GqeqCqNlXV\nDwE3M/pqZx6Yr6q7uu33MHgjOC6NOeafAP6pqg5W1X8D7wN+eCX72UeSExkEwruq6n1d8aNdwB0J\nugMjmo7z8SXHnB7jPTKV+UHgd6vqEyvR32noMebnAq9L8jCD32pfkeTaFejysjD4J5Dk9O71ScDv\nAtcvrFNV/wo8kuT8ruhi4DMr1skpG2fMDKZ4npPkKd3Hb1/MYB71mNf190ZgT1W9dWjX8MeObAE+\nMKL5bcCmJGu733o2dWXHrD7j7T6i5RbgHVX1d8vd12npM+aq+vmqWldV64FfZzD24/c3+NX+6/Kx\nvjC4ut0P/DeDK7urgNczuCPgc8C1/P+DcGcx+HXwSNsNwBxwH/B+jvE7PaY05t8HHmDwN4F3At+y\n2uMZc8w/wmB65j5gV7dsZvAx4ncAe7vXU7v6s8ANQ+1fDTzYLa9a7fEs53iBX+j+bewaWjas9piW\n+2c8dJxXcpzf1eOTu5LUGKd6JKkxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqjMEvSY35PzSW\nhzmLuEF9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "targetDf.groupBy(\"points\").count().rdd.values().histogram(5)\n",
    "#Matplotlib\n",
    "\n",
    "df_pd = targetDf.toPandas()\n",
    "plt.hist(df_pd['points'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
