{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ClusteringParallel.py\n"
     ]
    }
   ],
   "source": [
    "#Clustering\n",
    "%%file ClusteringParallel.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.sparse import vstack,csr_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "\n",
    "def readData():\n",
    "    '''Read the Tfid data from 20 newsgroup dataset'''\n",
    "    #cats = ['alt.atheism', 'sci.space','comp.graphics']\n",
    "    newsgroup_train = fetch_20newsgroups(subset = 'train',remove=('headers', 'footers', 'quotes'))#,categories=cats)\n",
    "    text = pd.DataFrame([str(i) for i in newsgroup_train.data])\n",
    "    ps = PorterStemmer()\n",
    "    #Stemming\n",
    "    text['stemmed'] = text.apply(lambda x: [ps.stem(y) for y in x])\n",
    "    #Tfid\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    vector = vectorizer.fit_transform(text.stemmed.dropna())\n",
    "    return(vector)\n",
    "\n",
    "def initialCentroid(x,k):\n",
    "    '''Setting the initial value for centroids'''\n",
    "    centroidIndex = list(np.random.randint(x.shape[0],size = k))\n",
    "    centroid = x[centroidIndex[0],:].todense()\n",
    "    #print(\"first\",first)\n",
    "    for i in centroidIndex[1:]:\n",
    "        temp = np.array(x[i,:].todense())\n",
    "        #print(\"temp\",temp)\n",
    "        centroid = np.concatenate((centroid,temp))\n",
    "    return(centroid)\n",
    "\n",
    "def distance(a,b):\n",
    "    '''Calculate the euclidean distance and return the closest cluster center'''\n",
    "    dist = np.sqrt(a.dot(b.T))\n",
    "    cluster = np.argmin(dist, axis=1)\n",
    "    return(np.array(cluster))\n",
    "\n",
    "def concatenate(partFile,dist,k):\n",
    "    '''Group the documents based on the clustering done for the first time'''\n",
    "    newSum = np.array([])\n",
    "    clusters = np.unique(dist)\n",
    "    partFile.tocsr()\n",
    "    for each in range(0,k):\n",
    "        if each in clusters:\n",
    "            points = scipy.sparse.vstack([partFile[j] for j in range(partFile.shape[0]) if dist[j] == each])\n",
    "        else:\n",
    "            points = partFile[each,:]\n",
    "        newSum = scipy.sparse.vstack((newSum,np.array(points.sum(axis = 0))))\n",
    "    newSum = (newSum.tocsr())[1:,]\n",
    "    return(newSum)\n",
    "\n",
    "def globalMean(partFile,dist,k):\n",
    "    '''Each worker returns the local sum and number of elements in each cluster, merging and computing global mean'''\n",
    "    newSum = np.array([])\n",
    "    actualCluster = np.unique(dist)\n",
    "    partFile.tocsr()\n",
    "    #print(\"Actual \",actualCluster)\n",
    "    #print(\"Update centroid \", partFile.shape)\n",
    "    for each in range(0,k):\n",
    "        if each in actualCluster:\n",
    "            for j in range(partFile.shape[0]):\n",
    "                if dist[j] == each:\n",
    "                    points = scipy.sparse.vstack(partFile[j])\n",
    "            dnr = dist.count(each)\n",
    "        else:\n",
    "            dnr =1\n",
    "            points = partFile[each]\n",
    "        newSum = scipy.sparse.vstack((newSum,np.array(points.sum(axis = 0))))/dnr\n",
    "            \n",
    "    newSum = (newSum.tocsr())[1:,]\n",
    "    return(newSum)\n",
    "\n",
    "def dataDistribute(data,centroid):\n",
    "    '''All the send functions in root process'''\n",
    "    rows = data.shape[0]\n",
    "    for i in range(1,size):\n",
    "        startIndex = int((i-1)*(rows/(size-1)))\n",
    "        endIndex = int(((rows/(size-1))*i))\n",
    "        ##print(\"Index\",startIndex,endIndex)\n",
    "        #May cause index out of range due to index strting from 0, but we are counting from 1\n",
    "        if endIndex > rows:\n",
    "            endIndex = int(rows)\n",
    "        comm.send(data[startIndex:endIndex,:],dest = i,tag =1)\n",
    "        comm.send(centroid, dest = i,tag =2)\n",
    "\n",
    "def dataCollect(k):\n",
    "    '''All the receive functions in root process'''\n",
    "    updatedCentroid,updatedCluster,meanCluster = np.array([]),[],[]\n",
    "    for i in range(1,size):\n",
    "        #print(\"data received from \",i)\n",
    "        updatedCentroid = scipy.sparse.vstack((updatedCentroid,comm.recv(source = i, tag = 3)))\n",
    "        updatedCluster.extend(comm.recv(source = i, tag = 4))\n",
    "        meanCluster.extend(comm.recv(source =i, tag =5))\n",
    "        updatedCentroid = (updatedCentroid.tocsr())[1:,]\n",
    "        finalSum = globalMean(updatedCentroid,updatedCluster,k)\n",
    "    print(\"At source centroid \",updatedCentroid.shape,\" partFile \", finalSum.shape, \"recived after update\")\n",
    "    return (finalSum,updatedCluster)\n",
    "\n",
    "def kMeans(data,centroid,k):\n",
    "    '''The function called recursively to compute KNN'''\n",
    "    comm.Barrier()\n",
    "    if rank == 0:\n",
    "        '''Root process'''\n",
    "        dataDistribute(data,centroid)\n",
    "        updatedCentroid,updatedCluster = dataCollect(k)\n",
    "        ##print(\"BRACE FOR IMPACT\")\n",
    "\n",
    "    else :\n",
    "        '''Workers'''\n",
    "        updatedCluster,updatedCentroid = None,None\n",
    "        partFile = comm.recv(source = 0, tag =1)\n",
    "        centroid = comm.recv(source = 0, tag =2)\n",
    "        #print(\"At rank \", rank, \"shape is \",partFile.shape)\n",
    "        #print(\"At rank \", rank, \"centroid \", centroid.shape[0])\n",
    "        dist = distance(partFile,centroid)\n",
    "        #print(\"At rank \", rank, \"dist \", dist.shape)\n",
    "        clusters = np.unique(dist)\n",
    "        #print(\"Clusters \",np.unique(dist))\n",
    "        newSum = concatenate(partFile,dist,k)\n",
    "        comm.send(newSum, dest = 0, tag =3)\n",
    "        comm.send(dist,dest = 0, tag = 4)\n",
    "        comm.send(clusters, dest = 0, tag = 5)\n",
    "        print(\"Data send from worker \", rank)\n",
    "    \n",
    "    comm.Barrier()\n",
    "    #Broadcasting the values to be returned or else they end up null in main function :(\n",
    "    updatedCluster = comm.bcast(updatedCluster, root = 0)\n",
    "    updatedCentroid = comm.bcast(updatedCentroid, root = 0)\n",
    "    comm.Barrier()\n",
    "    return(np.concatenate(updatedCluster,axis = 0),updatedCentroid)\n",
    "    comm.Barrier()\n",
    "    \n",
    "\n",
    "#Setting the communicators\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "def main():    \n",
    "    data = readData()\n",
    "    flag = True\n",
    "    cnt = 1\n",
    "    k = 20\n",
    "    #Random array to check compare the clusters after every iteration\n",
    "    default = np.array([k+1]*data.shape[0])\n",
    "    comm.Barrier()\n",
    "    #Getting initial centroids\n",
    "    centroid = initialCentroid(data,k)\n",
    "    #Start of parallel processing\n",
    "    comm.Barrier()\n",
    "    tic = time.time()\n",
    "    #Calling Kmeans function for first time to get intial clusters\n",
    "    updatedCluster,updatedCentroid = kMeans(data,centroid,k)\n",
    "    print(\"Centroid at first \",np.unique(updatedCluster))\n",
    "    comm.Barrier()   \n",
    "    #Looping until membership remains the same\n",
    "    while flag == True:\n",
    "        print(\"Recomputing Centroids, iteration \",cnt,(updatedCluster == default).all())\n",
    "        #Checking for membership\n",
    "        if cnt ==500:\n",
    "            print(\"Algorithm did not converge in 1000 iterations\")\n",
    "            print(\"Time taken : \", time.time()-tic)\n",
    "            return()\n",
    "            break\n",
    "        if (updatedCluster==default).all():\n",
    "            print(\"Algorithm Converged\")\n",
    "            print(\"Time taken : \", time.time()-tic)\n",
    "            flag = False\n",
    "        else:\n",
    "            comm.Barrier()\n",
    "            #When condition fails recursively calling the kMeans function\n",
    "            updatedClusterNew,updatedCentroidNew = kMeans(data,updatedCentroid,k)\n",
    "            comm.Barrier()\n",
    "            print(\"Time taken : \", time.time()-tic)\n",
    "            cnt = cnt+1\n",
    "            print(\"Centroid shape in loop \", cnt ,np.unique(updatedCluster))\n",
    "        #Updating cluster memberships and centroids\n",
    "        default = updatedCluster\n",
    "        updatedCluster = updatedClusterNew\n",
    "        updatedCentroid = updatedCentroidNew\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    '''Main function'''\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 2 python ClusteringParallel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Clustering1.py\n"
     ]
    }
   ],
   "source": [
    "%%file ClusteringSerial.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from scipy.sparse import vstack,csr_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "\n",
    "def readData():\n",
    "    '''Read the Tfid data from 20 newsgroup dataset'''\n",
    "    #cats = ['alt.atheism', 'sci.space','comp.graphics']\n",
    "    newsgroup_train = fetch_20newsgroups(subset = 'train',remove=('headers', 'footers', 'quotes'))#,categories=cats)\n",
    "    text = pd.DataFrame([str(i) for i in newsgroup_train.data])\n",
    "    ps = PorterStemmer()\n",
    "    #Stemming\n",
    "    text['stemmed'] = text.apply(lambda x: [ps.stem(y) for y in x])\n",
    "    #Tfid\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    vector = vectorizer.fit_transform(text.stemmed.dropna())\n",
    "    return(vector)\n",
    "\n",
    "def initialCentroid(x,k):\n",
    "    '''Setting the initial value for centroids'''\n",
    "    centroidIndex = list(np.random.randint(x.shape[0],size = k))\n",
    "    centroid = x[centroidIndex[0],:].todense()\n",
    "    #print(\"first\",first)\n",
    "    for i in centroidIndex[1:]:\n",
    "        temp = np.array(x[i,:].todense())\n",
    "        #print(\"temp\",temp)\n",
    "        centroid = np.concatenate((centroid,temp))\n",
    "    return(centroid)\n",
    "\n",
    "def distance(a,b):\n",
    "    '''Calculate the euclidean distance and return the closest cluster center'''\n",
    "    dist = np.sqrt(a.dot(b.T))\n",
    "    cluster = np.argmin(dist, axis=1)\n",
    "    return(np.array(cluster))\n",
    "\n",
    "def concatenate(partFile,dist,k):\n",
    "    '''Group the documents based on the clustering done for the first time'''\n",
    "    newSum = np.array([])\n",
    "    clusters = np.unique(dist)\n",
    "    partFile.tocsr()\n",
    "    for each in range(0,k):\n",
    "        if each in clusters:\n",
    "            points = scipy.sparse.vstack([partFile[j] for j in range(partFile.shape[0]) if dist[j] == each])\n",
    "        else:\n",
    "            points = partFile[each,:]\n",
    "        newSum = scipy.sparse.vstack((newSum,np.array(points.sum(axis = 0))))\n",
    "    newSum = (newSum.tocsr())[1:,]\n",
    "    return(newSum)\n",
    "\n",
    "def globalMean(partFile,dist,k):\n",
    "    '''Each worker returns the local sum and number of elements in each cluster, merging and computing global mean'''\n",
    "    newSum = np.array([])\n",
    "    actualCluster = np.unique(dist)\n",
    "    partFile.tocsr()\n",
    "    #print(\"Actual \",actualCluster)\n",
    "    #print(\"Update centroid \", partFile.shape)\n",
    "    for each in range(0,k):\n",
    "        if each in actualCluster:\n",
    "            for j in range(partFile.shape[0]):\n",
    "                if dist[j] == each:\n",
    "                    points = scipy.sparse.vstack(partFile[j])\n",
    "            dnr = dist.count(each)\n",
    "        else:\n",
    "            dnr =1\n",
    "            points = partFile[each]\n",
    "        newSum = scipy.sparse.vstack((newSum,np.array(points.sum(axis = 0))))/dnr\n",
    "            \n",
    "    newSum = (newSum.tocsr())[1:,]\n",
    "    return(newSum)\n",
    "\n",
    "def kMeans(data,centroid,k):\n",
    "    '''The function called recursively to compute KNN'''\n",
    "    comm.Barrier()\n",
    "    if rank == 0:\n",
    "        '''Root process'''\n",
    "        for i in range(1,size):\n",
    "            '''All the send functions in root process'''\n",
    "            rows = data.shape[0]\n",
    "            startIndex = int((i-1)*(rows/(size-1)))\n",
    "            endIndex = int(((rows/(size-1))*i))\n",
    "            ##print(\"Index\",startIndex,endIndex)\n",
    "            #May cause index out of range due to index strting from 0, but we are counting from 1\n",
    "            if endIndex > rows:\n",
    "                endIndex = int(rows)\n",
    "            comm.send(data[startIndex:endIndex,:],dest = i,tag =1)\n",
    "            comm.send(centroid, dest = i,tag =2)\n",
    "            ##print(\"BRACE FOR IMPACT\")\n",
    "            '''All the receive functions in root process'''\n",
    "            updatedCentroid,updatedCluster,meanCluster = np.array([]),[],[]\n",
    "            #print(\"data received from \",i)\n",
    "            updatedCentroid = scipy.sparse.vstack((updatedCentroid,comm.recv(source = i, tag = 3)))\n",
    "            updatedCluster.extend(comm.recv(source = i, tag = 4))\n",
    "            meanCluster.extend(comm.recv(source =i, tag =5))\n",
    "            updatedCentroid = (updatedCentroid.tocsr())[1:,]\n",
    "            finalSum = globalMean(updatedCentroid,updatedCluster,k)\n",
    "            #print(\"At source centroid \",updatedCentroid.shape,\" partFile \", finalSum.shape, \"recived after update\")\n",
    "\n",
    "    else :\n",
    "        '''Workers'''\n",
    "        updatedCluster,updatedCentroid = None,None\n",
    "        partFile = comm.recv(source = 0, tag =1)\n",
    "        centroid = comm.recv(source = 0, tag =2)\n",
    "        #print(\"At rank \", rank, \"shape is \",partFile.shape)\n",
    "        #print(\"At rank \", rank, \"centroid \", centroid.shape[0])\n",
    "        dist = distance(partFile,centroid)\n",
    "        #print(\"At rank \", rank, \"dist \", dist.shape)\n",
    "        clusters = np.unique(dist)\n",
    "        #print(\"Clusters \",np.unique(dist))\n",
    "        newSum = concatenate(partFile,dist,k)\n",
    "        comm.send(newSum, dest = 0, tag =3)\n",
    "        comm.send(dist,dest = 0, tag = 4)\n",
    "        comm.send(clusters, dest = 0, tag = 5)\n",
    "        print(\"Data send from worker \", rank)\n",
    "    \n",
    "    comm.Barrier()\n",
    "    #Broadcasting the values to be returned or else they end up null in main function :(\n",
    "    updatedCluster = comm.bcast(updatedCluster, root = 0)\n",
    "    updatedCentroid = comm.bcast(updatedCentroid, root = 0)\n",
    "    comm.Barrier()\n",
    "    return(np.concatenate(updatedCluster,axis = 0),updatedCentroid)\n",
    "    comm.Barrier()\n",
    "    \n",
    "\n",
    "#Setting the communicators\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "def main():    \n",
    "    data = readData()\n",
    "    flag = True\n",
    "    cnt = 1\n",
    "    k = 20\n",
    "    #Random array to check compare the clusters after every iteration\n",
    "    default = np.array([k+1]*data.shape[0])\n",
    "    comm.Barrier()\n",
    "    #Getting initial centroids\n",
    "    centroid = initialCentroid(data,k)\n",
    "    #Start of parallel processing\n",
    "    comm.Barrier()\n",
    "    tic = time.time()\n",
    "    #Calling Kmeans function for first time to get intial clusters\n",
    "    updatedCluster,updatedCentroid = kMeans(data,centroid,k)\n",
    "    print(\"Centroid at first \",np.unique(updatedCluster))\n",
    "    comm.Barrier()   \n",
    "    #Looping until membership remains the same\n",
    "    while flag == True:\n",
    "        print(\"Recomputing Centroids, iteration \",cnt,(updatedCluster == default).all())\n",
    "        #Checking for membership\n",
    "        if cnt == 500:\n",
    "            print(\"Algorithm did not converge in 1000 iterations\")\n",
    "            print(\"Time taken : \", time.time()-tic)\n",
    "            return()\n",
    "            break\n",
    "        if (updatedCluster==default).all():\n",
    "            print(\"Algorithm Converged\")\n",
    "            flag = False\n",
    "        else:\n",
    "            comm.Barrier()\n",
    "            #When condition fails recursively calling the kMeans function\n",
    "            updatedClusterNew,updatedCentroidNew = kMeans(data,updatedCentroid,k)\n",
    "            comm.Barrier()\n",
    "            print(\"Time taken : \", time.time()-tic)\n",
    "            cnt = cnt+1\n",
    "            print(\"Centroid shape in loop \", cnt ,np.unique(updatedCluster))\n",
    "        #Updating cluster memberships and centroids\n",
    "        default = updatedCluster\n",
    "        updatedCluster = updatedClusterNew\n",
    "        updatedCentroid = updatedCentroidNew\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    '''Main function'''\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 2 python ClusteringSerial.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
