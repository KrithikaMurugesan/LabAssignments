{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: Basic Parallel Vector Operations with MPI (5Points)\n",
    "    \n",
    "    Suppose you are given a vector v ∈ R N . Initialize your vector v with random numbers (can be either inte-\n",
    "gers or floating points). You will experiment with three different sizes of vector, i.e. N = {10 7 , 10 12 , 10 15 }.\n",
    "You have to parallelize vector operations mentioned below using MPI API. For each operation you will\n",
    "have to run experiment with varying number of workers, i.e. if your system has P workers than run\n",
    "experiments with workers = {1, 2, . . . P } for each size of vector given above. You have to time your\n",
    "code for each operation and present it in a table. [Note: You have to define/explain your parallelization\n",
    "strategy i.e. how you assign task to each worker, how you divide your data etc.]. You have to use MPI\n",
    "point-to-point communication i.e. Send and Recv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Add two vectors and store results in a third vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Ex1a.py\n",
    "'''Exercise 1: Basic Parallel Vector Operations with MPI'''\n",
    "'''a) Add two vectors and store results in a third vector'''\n",
    "\n",
    "#Import numpy,mpi4py and time packages\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "#Setting alias to MPI.COMM_WORLD for easier reference in the code\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "#Computing the number of processes(size) and rank(current process)\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(comm,size,rank)\n",
    "resultVec = []\n",
    "\n",
    "#Function to through the different receiver processes to send the data\n",
    "def broadcast(data1,data2,size):\n",
    "    for i in range(1,size):\n",
    "        #Calculate processing time at each process\n",
    "        start = MPI.Wtime()\n",
    "        #Evenly spacing the vector among the available workers\n",
    "        startIndex = int((i-1)*(len(data1)/(size-1)))\n",
    "        endIndex = int(((len(data1)/(size-1))*i))\n",
    "        ##print(\"Index\",startIndex,endIndex)\n",
    "        #May cause index out of range due to index strting from 0, but we are counting from 1\n",
    "        if endIndex > len(data1):\n",
    "            endIndex = int(len(data1))\n",
    "        ##print(\"Data Sent :\",data[startIndex:endIndex])\n",
    "        #Sending parts of vectors to the process\n",
    "        comm.send(data1[startIndex:endIndex],dest = i)\n",
    "        comm.send(data2[startIndex:endIndex],dest = i)\n",
    "        #Receiving answer\n",
    "        tempVec = comm.recv(source = i)\n",
    "        #Storing results from all processes in a new vector\n",
    "        resultVec.append(tempVec.tolist())\n",
    "        stop = MPI.Wtime() \n",
    "        print(\"Time required :\",stop-start)\n",
    "    #print(\"Vector with sum : \",sum(resultVec,[]))\n",
    "\n",
    "        \n",
    "vecSize = 10**5\n",
    "#Consider worker 0 to initialize the data and broadcast to other workers\n",
    "def vectorAddition():\n",
    "    if rank == 0:\n",
    "        #Initializing vector ar root process\n",
    "        vec1 = np.random.rand(vecSize)\n",
    "        vec2 = np.random.rand(vecSize)\n",
    "        #print(\"original\",vec1,\"\\n\",vec2)\n",
    "        #Using send and recv to distribute vectors to all processes\n",
    "        broadcast(vec1,vec2,size)\n",
    "    \n",
    "    \n",
    "#For all workers except 0, receiving the data sent by source\n",
    "    else :\n",
    "        dataRecv1 = comm.recv(source = 0)\n",
    "        dataRecv2 = comm.recv(source = 0)\n",
    "        print(\"Vector size at {} is {}\".format(rank,len(dataRecv1)))\n",
    "        #Computing the sum and sending it to root process\n",
    "        comm.send(dataRecv1+dataRecv2,dest = 0)\n",
    "\n",
    "#To ensure synchronization\n",
    "comm.barrier()\n",
    "#Time with overheads(waiting time)\n",
    "tic = MPI.Wtime()\n",
    "vectorAddition()\n",
    "#End of process, stop timer\n",
    "comm.barrier()\n",
    "tac = MPI.Wtime()\n",
    "\n",
    "print(\"Total time \", tac-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 3 python Ex1a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Find an average of numbers in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Ex1b.py\n",
    "#Import numpy,mpi4py and time packages\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "#Setting alias to MPI.COMM_WORLD for easier reference in the code\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "#Computing the number of processes(size) and rank(current process)\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(comm,size,rank)\n",
    "\n",
    "\n",
    "vecSize = 10**7\n",
    "result = []\n",
    "\n",
    "#Function to broadcast vectors to all receiving processes\n",
    "def broadcastSum(data,size):\n",
    "    \n",
    "    for i in range(1,size):\n",
    "        tic = time.time()\n",
    "        #Evenly spacing the vector among the available workers\n",
    "        startIndex = int((i-1)*(len(data)/(size-1)))\n",
    "        endIndex = int(((len(data)/(size-1))*i))\n",
    "        ##print(\"Index\",startIndex,endIndex)\n",
    "        if endIndex > len(data):\n",
    "            endIndex = int(len(data))\n",
    "        ##print(\"Data Sent :\",data[startIndex:endIndex])\n",
    "        #Sending partitioned data to available processes\n",
    "        comm.send(data[startIndex:endIndex],dest = i)\n",
    "        temp = comm.recv(source = i)\n",
    "        result.append(temp)\n",
    "        tac = time.time()\n",
    "        print(\"Time without overhead \",tac-tic)\n",
    "    print(\"Average\",np.sum(np.array(result))/vecSize)\n",
    "    \n",
    "\n",
    "#Capture initial time\n",
    "comm.barrier()\n",
    "start = time.time()\n",
    "\n",
    "#Consider worker 0 to initialize the data and broadcast to other workers\n",
    "if rank == 0:\n",
    "    vec = np.random.rand(vecSize)\n",
    "    #print(\"original\",vec)\n",
    "    broadcastSum(vec,size)\n",
    "    \n",
    "else:\n",
    "    dataRecv = comm.recv(source = 0)\n",
    "    comm.send(np.sum(np.array(dataRecv)),dest = 0)\n",
    "    \n",
    "\n",
    "#End of process, stop timer   \n",
    "comm.barrier()\n",
    "stop = time.time() \n",
    "\n",
    "print(\"Time required :\",stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 4 python Ex1b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Parallel Matrix Vector multiplication using MPI (7 Points)\n",
    "\n",
    "        In this exercise you have to work with a matrix A ∈ R N ×N and two vectors b ∈ R N , c ∈ R N . Initialize\n",
    "the matrix A and vector b with random numbers (can be either integers or floating points). The vector\n",
    "c will store result of A × b.\n",
    "In case of matrix vector multiplication, you will experiment with three different sizes of matrices i.e.\n",
    "N = {10 2 , 10 3 , 10 4 }. [note: your matrix will be N × N , which means in case 1 you will have matrix\n",
    "dimension 100x100]. You will have to run experiments with varying number of workers, i.e. if your system\n",
    "has P workers than run experiments with workers = {1, 2, . . . P } for each matrix size given above. You\n",
    "have to time your code and present it in a table.\n",
    "Implement parallel matrix vector multiplication using MPI point-to-point communication i.e. Send\n",
    "and Recv. Explain your logic in the report i.e. how the matrix and vectors are divided (distributed)\n",
    "among workers, what is shared among them, how is the work distributed, what individual worker will\n",
    "do and what master worker will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Ex2.py\n"
     ]
    }
   ],
   "source": [
    "%%file Ex2.py\n",
    "#Import numpy,mpi4py and time packages\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "n = 10**4\n",
    "product = []\n",
    "    \n",
    "\n",
    "#Setting alias to MPI.COMM_WORLD for easier reference in the code\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "#Computing the number of processes(size) and rank(current process)\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print(comm,size,rank)\n",
    "\n",
    "#Function to broadcast vectors to all receiving processes\n",
    "def broadcast(matrix,vector,size):\n",
    "    for i in range(1,size):\n",
    "        \n",
    "        #Evenly spacing the vector among the available workers\n",
    "        startIndex = int((i-1)*(matrix.shape[0]/(size-1)))\n",
    "        endIndex = int(((matrix.shape[0]/(size-1))*i))\n",
    "        ##print(\"Index\",startIndex,endIndex)\n",
    "        if endIndex > matrix.shape[0]:\n",
    "            endIndex = int(matrix.shape[0])\n",
    "        ##print(\"Data Sent :\",data[startIndex:endIndex])\n",
    "        tic = time.time()\n",
    "        comm.send(matrix[startIndex:endIndex,],dest = i)\n",
    "        comm.send(vector,dest = i)\n",
    "        temp = comm.recv(source = i)\n",
    "        product.append(temp.tolist())\n",
    "        print(\"Time without overhead \",time.time()-tic)\n",
    "    #print(\"Result\",sum(product,[]))\n",
    "\n",
    "\n",
    "def process():\n",
    "#Consider worker 0 to initialize the data and broadcast to other workers\n",
    "    if rank == 0:\n",
    "        matrix = np.random.rand(n,n)\n",
    "        #print(\"Matrix\",matrix)\n",
    "        vector = np.random.rand(n)\n",
    "        #print(\"Vector\",vector)\n",
    "        broadcast(matrix,vector,size)\n",
    "    \n",
    "    else:\n",
    "        matrixRecv = comm.recv(source = 0)\n",
    "        vectorRecv = comm.recv(source = 0)\n",
    "        part = np.matmul(matrixRecv,vectorRecv)\n",
    "        comm.send(part,dest = 0)\n",
    "    \n",
    "#Capture initial time\n",
    "comm.barrier()\n",
    "start = time.time()\n",
    "process()\n",
    "#End of process, stop timer  \n",
    "comm.barrier()\n",
    "stop = time.time() \n",
    "print(\"Time required :\",stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mpi4py.MPI.Intracomm object at 0x7f5ba43c9ac8> 4 0\r\n",
      "Time without overhead  0.6799919605255127\r\n",
      "Time without overhead  0.6668262481689453\r\n",
      "Time without overhead  0.6557085514068604\r\n",
      "Time required : 4.138477563858032\r\n",
      "<mpi4py.MPI.Intracomm object at 0x7faa4860bac8> 4 1\r\n",
      "Time required : 4.1384875774383545\r\n",
      "<mpi4py.MPI.Intracomm object at 0x7fe2c1189ac8> 4 2\r\n",
      "Time required : 4.1384899616241455\r\n",
      "<mpi4py.MPI.Intracomm object at 0x7fe381da4ac8> 4 3\r\n",
      "Time required : 4.138485908508301\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python Ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: Parallel Matrix Operation using MPI (8 Points)\n",
    "    \n",
    "    In this exercise you have to work with three matrices (A ∈ R N ×N , B ∈ R N ×N , C ∈ R N ×N ) i.e each\n",
    "matrix having size N ×N . Initialize your matrices A and B with random numbers (can be either integers\n",
    "or floating points). Matrix C will store result of A × B.\n",
    "In case of matrix multiplication, you will experiment with three different sizes of matrices i.e. N =\n",
    "{10 2 , 10 3 , 10 4 }. [note: your matrix will be N × N , which means in case 1 you will have matrices with\n",
    "dimension 100x100]. You will have to run experiments with varying number of workers, i.e. if your system\n",
    "has P workers than run experiments with workers = {1, 2, . . . P } for each matrix size given above. You\n",
    "have to time your code and present it in a table.\n",
    "Implement parallel matrix matrix multiplication using MPI collective communication. Explain your\n",
    "logic in the report i.e. how the matrices are divided (distributed) among workers, what is shared among\n",
    "them, how is the work distributed, what individual worker will do and what master worker will do. Per-\n",
    "form experiments with varying matrix sizes and varying number of workers. You can look at the imple-\n",
    "mentation provided in the lecture (slide 48) https://www.ismll.uni-hildesheim.de/lehre/bd-17s/\n",
    "script/bd-01-parallel-computing.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Ex3.py\n"
     ]
    }
   ],
   "source": [
    "%%file Ex3.py\n",
    "#Importing the required packages\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "row =10**2\n",
    "col =10**2\n",
    "pdt = None\n",
    "\n",
    "sendbuf = None\n",
    "\n",
    "tic = MPI.Wtime()\n",
    "#Prcoesses to be done at root\n",
    "if rank == 0:\n",
    "    sendbuf = np.random.rand(row,col)\n",
    "    matrixB = np.random.rand(row,col)\n",
    "    \n",
    "    #Calculating split and displacements for ScatterV and GatherV as scatter and gather cannot deal when\n",
    "    #number of rows cannot be evenly distributed to workers \n",
    "    split = np.array_split(sendbuf,size,axis=0)\n",
    "    #print(\"split\",type(split))\n",
    "    splits = []\n",
    "\n",
    "    for i in range(0,len(split),1):\n",
    "        splits = np.append(splits, len(split[i]))\n",
    "        \n",
    "    splitI = splits*col\n",
    "    dispI = np.insert(np.cumsum(splitI),0,0)[0:-1]\n",
    "\n",
    "    splitO = splits*col\n",
    "    dispO = np.insert(np.cumsum(splitO),0,0)[0:-1]\n",
    "    \n",
    "    #print(\"Mat A\",sendbuf)\n",
    "    #print(\"Mat B\",matrixB)\n",
    "    matrixA = np.empty((int(row/size),col))\n",
    "    pdt = np.empty((row,col))\n",
    "    #print(\"rank o \",sendbuf)\n",
    "    \n",
    "else:\n",
    "    splitI = None\n",
    "    dispI = None\n",
    "    splitO = None\n",
    "    dispO = None\n",
    "    split = None\n",
    "    \n",
    "    matrixB = np.empty((row,col))\n",
    "\n",
    "#Bcasting all variables for scatter and gather as they were computed only in root process\n",
    "split = comm.bcast(split, root=0) \n",
    "splitI = comm.bcast(splitI, root = 0)\n",
    "dispI = comm.bcast(dispI, root = 0)\n",
    "splitO = comm.bcast(splitO, root = 0)\n",
    "dispO = comm.bcast(dispO, root = 0)\n",
    "\n",
    "#print(\"After bcast\",type(split),type(rank),type(col))\n",
    "matrixA = np.empty((split[rank].shape[0],col))\n",
    "\n",
    "#Scattering matrix A\n",
    "comm.Scatterv([sendbuf,splitI, dispI,MPI.DOUBLE], matrixA, root=0)\n",
    "#Bcasting matrix B\n",
    "matrixB = comm.bcast(matrixB, root = 0)\n",
    "\n",
    "\n",
    "comm.barrier()\n",
    "#Performing matrix multiplication at all processes including root\n",
    "temp = np.matmul(matrixA,matrixB)\n",
    "#Gathering the results at root process\n",
    "comm.Gatherv(temp,[pdt,splitO,dispO,MPI.DOUBLE], root=0)\n",
    "\n",
    "comm.barrier()\n",
    "toc = MPI.Wtime()\n",
    "\n",
    "#print(\"rank\",rank)\n",
    "print(\"Time at Rank\",rank,\"Time\",toc-tic)\n",
    "#print(\"Scatter\",matrixA)\n",
    "#print(\"Bcast\",matrixB)\n",
    "#print(\"Pdt\",pdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time at Rank 0 Time 0.022871017456054688\r\n",
      "Time at Rank 1 Time 0.01910400390625\r\n",
      "Time at Rank 2 Time 0.05176091194152832\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 3 python Ex3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MPI.Scatterv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.20702242851257324\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "n = 10**3\n",
    "\n",
    "tic = time.time()\n",
    "if rank == 0:\n",
    "    vmatrix = np.random.rand(n,n)\n",
    "    #print(\"Matrix\",matrix)\n",
    "    vector = np.random.rand(n,n)\n",
    "    new = np.matmul(vmatrix,vector)\n",
    "print(\"Time\",time.time()-tic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
