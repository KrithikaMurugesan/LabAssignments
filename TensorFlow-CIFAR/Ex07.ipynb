{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data and labels\n",
    "def readData(file):\n",
    "    from six.moves import cPickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo,encoding='latin1')\n",
    "        data = dict['data']\n",
    "        labels = dict['labels']\n",
    "    return(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "def trainTest():\n",
    "    x,y = readData(\"/home/kritz/Downloads/cifar-10-python/cifar-10-batches-py/data_batch_1\")\n",
    "    \n",
    "    '''Using only one batch due to less computation power available'''\n",
    "    #for i in range(2,6):\n",
    "        #xTemp,yTemp = readData(\"/home/kritz/Downloads/cifar-10-python/cifar-10-batches-py/data_batch_\"+str(i))\n",
    "        #x = np.append(x, xTemp, axis=0)\n",
    "        #y = np.append(y, yTemp, axis=0)\n",
    "\n",
    "    y = np.array(y)\n",
    "    y = np.reshape(y,((len(y),1)))\n",
    "\n",
    "    #Test data\n",
    "    testX,testY = readData(\"/home/kritz/Downloads/cifar-10-python/cifar-10-batches-py/test_batch\")\n",
    "    testY = np.array(testY)\n",
    "    testY = np.reshape(testY,((len(testY),1)))\n",
    "\n",
    "    cat = 32\n",
    "    imgSize = 32\n",
    "    imgChannels = 3\n",
    "\n",
    "    #Seperating data into pixels and color channel\n",
    "    x = x.reshape([-1, imgChannels, imgSize, cat])\n",
    "    testX = testX.reshape([-1, imgChannels, imgSize, cat])\n",
    "    \n",
    "    #ONe hot encoding for output variables\n",
    "    encodedTrain = np.zeros((len(y), 10))\n",
    "    for idx, val in enumerate(y):\n",
    "        encodedTrain[idx][val] = 1\n",
    "        \n",
    "    encodedTest = np.zeros((len(testY), 10))\n",
    "    for idx, val in enumerate(testY):\n",
    "        encodedTest[idx][val] = 1\n",
    "    \n",
    "    return (testX,encodedTest,x,encodedTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate image\n",
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (3,32,32))\n",
    "    k = tf.placeholder(tf.int32)\n",
    "    tf_img = tf.image.rot90(X, k = 2)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            rotated_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "            X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return X_rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize Image\n",
    "def tfResize(trainX):\n",
    "    IMAGE_SIZE = 20\n",
    "    X_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (3,None, None))\n",
    "    tf_img = tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Each image is resized individually as different image may be of different size.\n",
    "        for each in trainX:\n",
    "            resized_img = sess.run(tf_img, feed_dict = {X: each})\n",
    "            X_data.append(resized_img)\n",
    "\n",
    "    X_data = np.array(resized_img, dtype = np.float32) # Convert to numpy\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reading the test and train data\n",
    "testX,testY,trainX,trainY = trainTest()\n",
    "\n",
    "print(\"Train, test done! \")\n",
    "\n",
    "\n",
    "#Looking at a sample image\n",
    "rgb = trainX[8]\n",
    "img = rgb.reshape(3,32,32).transpose([1, 2, 0])\n",
    "plt.title(\"Original image\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(img.shape)\n",
    "\n",
    "#2.Rotating the images\n",
    "rotated = rotate_images(trainX)\n",
    "rgb = rotated[8]\n",
    "img = rgb.reshape(3,32,32).transpose([1, 2, 0])\n",
    "plt.title(\"Rotated image\")\n",
    "plt.imshow(img/255)\n",
    "plt.show()\n",
    "\n",
    "#3.Resize\n",
    "resized = tfResize(trainX.transpose([0, 2, 3, 1]))\n",
    "rgb = resized\n",
    "img = rgb.reshape(3,20,20).transpose([1, 2, 0])\n",
    "plt.title(\"Resized Image\")\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "trainX = trainX.transpose([0, 2, 3, 1])\n",
    "testX = testX.transpose([0, 2, 3, 1])\n",
    "trainX = np.concatenate((trainX, rotated), axis=0)\n",
    "trainX = np.concatenate((trainX, resized), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 : with Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex_1_Norm\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result1a = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result1a.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "    result1a = np.array(result1a)\n",
    "    np.savetxt('Ex1_Norm.csv',result1a, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1 :Without Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    #conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_pool, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    #conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_pool)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex71b_noNorm\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result1b = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result1b.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "    result1b = np.array(result1b)\n",
    "    np.savetxt('Ex1b_NoNorm.csv',result1b, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 : with DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, 0.5)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, 0.5)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex7_2_DropOut\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer =    tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result2a = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result2a.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "    result2a = np.array(result2a)\n",
    "    np.savetxt('Ex2a_DropOut.csv',result2a, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 : without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex7_2_b_NoDrop\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result2b = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result2b.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "    result2b = np.array(result2b)\n",
    "    np.savetxt('Ex2b_NoDropOut.csv',result2b, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: RMSProp Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, 0.5)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, 0.5)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex73a_RMS\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result3a = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result3a.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "    result3a = np.array(result3a)\n",
    "    np.savetxt('Ex3a_DropOut.csv',result3a, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exercise 3 : Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = testX[1000:3000,]\n",
    "testY = testY[1000:3000,]\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, 0.5)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, 0.5)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=10, activation_fn=None)\n",
    "    out = tf.nn.softmax(out)\n",
    "    return out\n",
    "\n",
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer,feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "epochs = 5\n",
    "batchSize = 1200\n",
    "keep_probability = 0.0001\n",
    "learning_rate = 0.001\n",
    "logs_path = \"/tmp/cnn/Ex73a_Adam\"\n",
    "\n",
    "logits = conv_net(x, keep_prob)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "\n",
    "# Loss and Optimizer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    \n",
    "tf.summary.histogram(\"cost\", cost)\n",
    "optimizer =   tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "accuracy_summary = tf.summary.scalar(\"Test Accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result3b = []\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = int(trainX.shape[0]/batchSize)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(logs_path + '/test', graph=tf.get_default_graph())\n",
    "    \n",
    "        for i in range(0, n_batches):\n",
    "            batchX = trainX[i*batchSize: (i+1)*batchSize]\n",
    "            batchY = trainY[i*batchSize: (i+1)*batchSize]\n",
    "            #train_neural_network(sess, optimizer, 0.000000000005,batchX,batchY )\n",
    "                \n",
    "            summary,_,co,accu = sess.run([summary_op,optimizer,cost,accuracy], feed_dict={x: batchX,y: batchY,keep_prob: 1.0})\n",
    "            summ,_,c,acc = sess.run([accuracy_summary,optimizer,cost,accuracy],feed_dict={x: testX,y: testY,keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            writer.add_summary(summ,i)\n",
    "            writer.flush()\n",
    "            temp = [epoch+1,i+1,co,accu,c,acc]\n",
    "            result3b.append(temp)\n",
    "            print('Epoch {:>2}, Batch {}:  '.format(epoch + 1, i), accu, acc, co, c)\n",
    "            \n",
    "    result2a = np.array(result3b)\n",
    "    np.savetxt('Ex3b_Adam.csv',result2a, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Performance graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames=['epoch', 'iterations', 'trainLoss', 'trainAcc','testLoss','testAcc']\n",
    "df1 = pd.read_csv(\"Ex1_Norm.csv\",delimiter = \",\",names=colnames, header = None)\n",
    "df2 = pd.read_csv(\"Ex1b_NoNorm.csv\",delimiter = \",\",names=colnames, header = None)\n",
    "df3 = pd.read_csv(\"Ex2a_DropOut.csv\",delimiter = \",\",names=colnames, header = None)\n",
    "df4 = pd.read_csv(\"Ex2b_NoDropOut.csv\",delimiter = \",\",names=colnames, header = None)\n",
    "df5 = pd.read_csv(\"Ex3a_DropOut.csv\",delimiter = \",\",names=colnames, header = None)\n",
    "df6 = pd.read_csv(\"Ex3b_Adam.csv\",delimiter = \",\",names=colnames, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(1,41)\n",
    "plt.plot(it,df1['trainAcc'],label = \"Norm\")\n",
    "plt.plot(it,df2['trainAcc'],label = \"No Norm\")\n",
    "plt.legend()\n",
    "plt.title(\"Normalization vs No Normalization Train Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df1['testAcc'],label = \"Norm\")\n",
    "plt.plot(it,df2['testAcc'],label = \"No Norm\")\n",
    "plt.legend()\n",
    "plt.title(\"Normalization vs No Normalization Test Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df1['trainLoss'],label = \"Norm\")\n",
    "plt.plot(it,df2['trainLoss'],label = \"No Norm\")\n",
    "plt.legend()\n",
    "plt.title(\"Normalization vs No Normalization Train Loss\")\n",
    "plt.show()\n",
    "plt.plot(it,df2['testLoss'],label = \"Norm\")\n",
    "plt.plot(it,df1['testLoss'],label = \"No Norm\")\n",
    "plt.legend()\n",
    "plt.title(\"Normalization vs No Normalization Test Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(1,41)\n",
    "plt.plot(it,df3['trainAcc'],label = \"DropOut\")\n",
    "plt.plot(it,df4['trainAcc'],label = \"No DropOut\")\n",
    "plt.legend()\n",
    "plt.title(\"DropOut vs No DropOut Train Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df3['testAcc'],label = \"DropOut\")\n",
    "plt.plot(it,df4['testAcc'],label = \"No DropOut\")\n",
    "plt.legend()\n",
    "plt.title(\"DropOut vs No DropOut Test Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df3['trainLoss'],label = \"DropOut\")\n",
    "plt.plot(it,df4['trainLoss'],label = \"No DropOut\")\n",
    "plt.legend()\n",
    "plt.title(\"DropOut vs No DropOut Train Loss\")\n",
    "plt.show()\n",
    "plt.plot(it,df3['testLoss'],label = \"DropOut\")\n",
    "plt.plot(it,df4['testLoss'],label = \"No DropOut\")\n",
    "plt.legend()\n",
    "plt.title(\"DropOut vs No DropOut Test Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = range(1,41)\n",
    "plt.plot(it,df5['trainAcc'],label = \"RMSProp\")\n",
    "plt.plot(it,df6['trainAcc'],label = \"Adam\")\n",
    "plt.legend()\n",
    "plt.title(\"RMSProp vs Adam Train Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df5['testAcc'],label = \"RMSProp\")\n",
    "plt.plot(it,df6['testAcc'],label = \"Adam\")\n",
    "plt.legend()\n",
    "plt.title(\"RMSProp vs Adam Test Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(it,df5['trainLoss'],label = \"RMSProp\")\n",
    "plt.plot(it,df6['trainLoss'],label = \"Adam\")\n",
    "plt.legend()\n",
    "plt.title(\"RMSProp vs Adam Train Loss\")\n",
    "plt.show()\n",
    "plt.plot(it,df5['testLoss'],label = \"RMSProp\")\n",
    "plt.plot(it,df6['testLoss'],label = \"Adam\")\n",
    "plt.legend()\n",
    "plt.title(\"RMSProp vs Adam Test Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=\"/tmp/cnn/Ex73a_RMS\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
