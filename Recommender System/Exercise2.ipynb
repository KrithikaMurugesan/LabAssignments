{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Recommender System using Apache Spark MLLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have to implement Recommender system using Apache Spark, we first read the data into an RDD, we delete the timestamp column as it is not factored in making suggestions. The ratings column is converted to a double type as the built in function supports only this datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up spark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|     31|   2.5|\n",
      "|     1|   1029|   3.0|\n",
      "|     1|   1061|   3.0|\n",
      "|     1|   1129|   2.0|\n",
      "|     1|   1172|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#Reading data\n",
    "rating = pd.read_csv(r'/home/kritz/Documents/DDL/Ex10/movieLens/ratings.csv')\n",
    "ratings = sqlContext.createDataFrame(rating)\n",
    "\n",
    "#Dropping timestamp as it is not necessary\n",
    "columns_to_drop = ['timestamp']\n",
    "rate = ratings.drop(*columns_to_drop)\n",
    "rate.show(5)\n",
    "\n",
    "#Renaming columns and converting to double type for function to use\n",
    "rate = rate.select(col(\"userId\").alias(\"user\"),col(\"movieId\")\n",
    "                                   .alias(\"item\"),col(\"rating\").alias(\"rating\"))\n",
    "newrate = rate.withColumn(\"rating\", rate[\"rating\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is prepared it is split into train data and test data, with 80% as train and remaining 20% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test split\n",
    "train,test = newrate.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make recommendations using matrix factorization method, we use the ALS function, which is Alternating Least Square matrix factorization.It trains a matrix factorization model given an RDD of ratings by users for a subset of products. The ratings matrix is approximated as the product of two lower-rank matrices of a given rank (number of features). To solve for these features, ALS is run iteratively with a configurable level of parallelism\n",
    "\n",
    "The hyper-parameters used in cross-validation are rank, maximum iterations, regularization parameter and alpha. The RMSE evalutor is used, i.e the cv minimizes the RMSE loss function, the best combination of parameters got from this cross-validation are used to make the predictions using which the RMSE is computed for Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_4c408ac6fee24df5c0ed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#ALS model\n",
    "alsImplicit = ALS(implicitPrefs=True)\n",
    "\n",
    "#Param grid for cv\n",
    "paramMapImplicit = ParamGridBuilder() \\\n",
    "                    .addGrid(alsImplicit.rank, [20.0,100.0])\\\n",
    "                    .addGrid(alsImplicit.maxIter, [10.0, 15.0]) \\\n",
    "                    .addGrid(alsImplicit.regParam, [0.01, 1.0]) \\\n",
    "                    .addGrid(alsImplicit.alpha, [10.0, 40.0]) \\\n",
    "                    .build()\n",
    "\n",
    "#RMSE Evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\n",
    "                                \",predictionCol=\"prediction\")\n",
    "\n",
    "#CV \n",
    "cvEstimator= CrossValidator(estimator=alsImplicit, \n",
    "                            estimatorParamMaps=paramMapImplicit, evaluator=evaluator)\n",
    "\n",
    "#Fitting CV\n",
    "cvModel=cvEstimator.fit(train)\n",
    "\n",
    "print(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error in Train data = 2.8648101624433124\n",
      "Root Mean Squared Error in Test data = 1.6589547812569874\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "predictions = cvModel.transform(train)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error in Train data = \" + str(rmse))\n",
    "\n",
    "# Evaluate the model on training data\n",
    "prediction = cvModel.transform(test)\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error in Test data = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It can be seen that the RMSE for test data is 1.66 which is a little more compared to the 0.98 baseline in \"http://www.mymedialite.net\". Since we are getting our own hyper paramters with the random split of data we make some deviations are bound to happen. Comparing with the values from the previous implementation these are much lesser RMSE values, as we have a wider combination of hyper-parameters being tested.The mediaLite is still the better model...and the predictions are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+----------+\n",
      "|user|item|rating|prediction|\n",
      "+----+----+------+----------+\n",
      "| 452| 463|   2.0|0.69587874|\n",
      "|  85| 471|   3.0|0.85212296|\n",
      "| 588| 471|   3.0|0.73714733|\n",
      "| 548| 471|   4.0|  1.001869|\n",
      "| 452| 471|   3.0| 0.3340096|\n",
      "+----+----+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "predsImplicit = cvModel.bestModel.transform(test)\n",
    "predsImplicit.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
