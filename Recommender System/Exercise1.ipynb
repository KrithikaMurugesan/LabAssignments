{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Recommender System from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To build a recommendation engine using matrix factorization, we read the data in an RDD and split the data into test and train using random split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark import SQLContext\n",
    "import pandas as pd\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Read data\n",
    "rating = pd.read_csv(r'/home/kritz/Documents/DDL/Ex10/movieLens/ratings.csv')\n",
    "ratings = sqlContext.createDataFrame(rating)\n",
    "\n",
    "#train test split\n",
    "Train,test = ratings.rdd.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To intialize matrix, we get the number of users and movies by getting the maximum of available values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 163950\n"
     ]
    }
   ],
   "source": [
    "#Size u and size v \n",
    "sizeU = Train.map(lambda x: int(x[0])).max() + 1\n",
    "sizeV = Train.map(lambda x: int(x[1])).max() + 1 \n",
    "\n",
    "print(sizeU,sizeV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to build a matrix with all the userId, movieId and actual ratings, we get the individual values and convert them to a matrix, we use a sparse matrix as the ratings are sparse in nature, a user does not rate all the available movies,a csr matrix is more efficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t2\n",
      "  (0, 1029)\t3\n",
      "  (0, 1061)\t3\n",
      "  (0, 1129)\t2\n",
      "  (0, 1172)\t4\n",
      "  (0, 1263)\t2\n",
      "  (0, 1287)\t2\n",
      "  (0, 1293)\t2\n",
      "  (0, 1343)\t2\n",
      "  (0, 1371)\t2\n",
      "  (0, 1405)\t1\n",
      "  (0, 1953)\t4\n",
      "  (0, 2105)\t4\n",
      "  (0, 2150)\t3\n",
      "  (0, 2193)\t2\n",
      "  (0, 2455)\t2\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "userId = Train.map(lambda x: int(x[0])).collect()\n",
    "movieId = Train.map(lambda x: int(x[1])).collect()\n",
    "rate = Train.map(lambda x: int(x[2])).collect()  \n",
    "\n",
    "#Getting matrix UV\n",
    "UV = csr_matrix((rate,(userId,movieId)), shape=(sizeU,sizeV))\n",
    "print(UV[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the same process for test data to get a test UV matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1339)\t3\n",
      "  (0, 2294)\t2\n",
      "  (0, 2968)\t1\n",
      "  (0, 3671)\t3\n"
     ]
    }
   ],
   "source": [
    "userId = test.map(lambda x: int(x[0])).collect()\n",
    "movieId = test.map(lambda x: int(x[1])).collect()\n",
    "rate = test.map(lambda x: int(x[2])).collect()  \n",
    "\n",
    "#Getting matrix UV\n",
    "uvTest = csr_matrix((rate,(userId,movieId)), shape=(sizeU,sizeV))\n",
    "print(uvTest[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to initialize the U and V matrices to random values to learn the matrix factorization with K latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Latent factors\n",
    "K = 20\n",
    "#Partititons\n",
    "P = 10\n",
    "\n",
    "#Initialize matrix\n",
    "uLatent = np.random.random_sample((sizeU,K))\n",
    "vLatent = np.random.random_sample((K,sizeV))\n",
    "\n",
    "#Partition train data\n",
    "trainParts = Train.repartition(P).persist()\n",
    "trainParts.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use RMSE Loss the function returns the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLoss(UV,mat):\n",
    "    mse = mean_squared_error(UV.todense(),mat)\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "def testLoss(test,mat):\n",
    "    rmse =mean_squared_error(test.collect(),mat)\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD function loops the iterations, it builds a smaller UV matrix for each partition in the data. This is used to update the uLatent nd vLatent matrices seperatly. These matrices are updated using the gradient of the loss function and returned to the main function where its average is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(partition,uLatent,vLatent,iters,alpha):\n",
    "    \n",
    "    userId,movieId,rating = [],[],[]\n",
    "    \n",
    "    for element in partition:\n",
    "        userId.append(element[0])   \n",
    "        movieId.append(element[1]) \n",
    "        rating.append(element[2])   \n",
    "        \n",
    "    users = np.amax(userId) +1   \n",
    "    movies = np.amax(movieId) +1 \n",
    "    \n",
    "    #create UV matrix with userIds and moviesIds present in partition\n",
    "    partUV = csr_matrix((rating,(userId,movieId)), shape=(users,movies))\n",
    "                                                                .todense()\n",
    "    \n",
    "    #non-zero elements\n",
    "    rows,cols = partUV.nonzero()\n",
    "    i = 0  \n",
    "    \n",
    "    #SGD\n",
    "    while i < iters:\n",
    "        #Computing gradient for a random sample\n",
    "        randomNum = random.randint(0,(rows).size-1)  \n",
    "        r = rows[randomNum]   \n",
    "        c = cols[randomNum]  \n",
    "        sample = partUV[r,c] \n",
    "\n",
    "        #compute gradients for U and V\n",
    "        temp1 = -2*(sample - uLatent[r,:].dot(vLatent[:,c]))*(vLatent[:,c].T)\n",
    "        uGrad = temp1 + (2*alpha*uLatent[r,:])/(partUV[r,:].nonzero()[0].size)\n",
    "        temp2 = -2*(sample - uLatent[r,:].dot(vLatent[:,c]))*(uLatent[r,:].T)\n",
    "        vGrad = temp2 + (2*alpha*vLatent[:,c])/(partUV[:,c].nonzero()[0].size)\n",
    "\n",
    "        #update the U and V values\n",
    "        uLatent[r,:] = uLatent[r,:] - alpha*uGrad \n",
    "        vLatent[:,c] = vLatent[:,c] - alpha*vGrad\n",
    "        i += 1    \n",
    "\n",
    "    return uLatent,vLatent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training function is where the data is sent to the SGD function, once all the part uLatent and vLatent matrices are calculated, their average is taken to get the final updated matrix. Persist is used to avoid memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(trainParts,alpha,uLatent,vLatent,UV,test):\n",
    "    #Training\n",
    "    #Learning rate\n",
    "    iters,epoch = 20,5  \n",
    "    i = 0\n",
    "    trainRmse,testRmse = [],[]\n",
    "\n",
    "    while i < epoch: \n",
    "        print(i)\n",
    "        updatedUV = trainParts.mapPartitions(lambda ratings_part: \n",
    "                        SGD(ratings_part,uLatent,vLatent,iters,alpha)).persist()\n",
    "        newUV = updatedUV.zipWithIndex().map(lambda x: (x[1]%2, x[0])\n",
    "                .reduceByKey(lambda r, k: r+k).map(lambda q: (q[1]/P)).persist()\n",
    "\n",
    "        #update uLatent\n",
    "        uLatent = newUV.collect()[0]\n",
    "\n",
    "        #update vLatent\n",
    "        vLatent = newUV.collect()[1]\n",
    "\n",
    "        #prediction\n",
    "        prediction = np.matmul(uLatent,vLatent)\n",
    "\n",
    "        #TrainLoss\n",
    "        TrainLoss= trainLoss(UV,prediction)\n",
    "        \n",
    "        #Validation loss\n",
    "        TestLoss = testLoss(test,prediction)\n",
    "        trainRmse.append(TrainLoss)\n",
    "        testRmse.append(TestLoss)\n",
    "        i = i+1\n",
    "    return(np.mean(testRmseLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation function is where the training data is split into 3 folds and all their combinations are trained, only two values of alpha are considered as the computation time is higher and efficiency. The alpha with lowest validation loss is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(alpha,trainOrig):\n",
    "    totLoss = []\n",
    "    #Splitting data into folds\n",
    "    parts = trainOrig.randomSplit([0.33, 0.33,0.34])\n",
    "    #Possible train and test combinations\n",
    "    trainSeq = [[0,1],[0,2],[1,2]]\n",
    "    testSeq = [2,1,0]\n",
    "    for i in range(0,3):\n",
    "        train = sc.union([parts[trainSeq[i][0]],parts[trainSeq[i][1]]])\n",
    "        Test = parts[testSeq[i]]\n",
    "    \n",
    "        #Size u and size v \n",
    "        sizeU = train.map(lambda x: int(x[0])).max() + 1\n",
    "        sizeV = train.map(lambda x: int(x[1])).max() + 1 \n",
    "\n",
    "        from scipy.sparse import csr_matrix\n",
    "\n",
    "        userId = train.map(lambda x: int(x[0])).collect()\n",
    "        movieId = train.map(lambda x: int(x[1])).collect()\n",
    "        rate = train.map(lambda x: int(x[2])).collect()  \n",
    "\n",
    "        #Getting matrix UV\n",
    "        UV = csr_matrix((rate,(userId,movieId)), shape=(sizeU,sizeV))\n",
    "\n",
    "        userId1 = Test.map(lambda x: int(x[0])).collect()\n",
    "        movieId1 = Test.map(lambda x: int(x[1])).collect()\n",
    "        rate1 = Test.map(lambda x: int(x[2])).collect()  \n",
    "\n",
    "        #Getting matrix UV\n",
    "        uvTest = csr_matrix((rate1,(userId1,movieId1)), shape=(sizeU,sizeV))\n",
    "        #print(uvTest[1])\n",
    "\n",
    "        #Latent factors\n",
    "        K = 20\n",
    "        #Partititons\n",
    "        P = 10\n",
    "\n",
    "        #Initialize matrix\n",
    "        uLatent = np.random.random_sample((sizeU,K))\n",
    "        vLatent = np.random.random_sample((K,sizeV))\n",
    "\n",
    "        #Partition train data\n",
    "        trainParts = train.repartition(P).persist()\n",
    "        trainParts.collect\n",
    "        \n",
    "        loss = training(trainParts,alpha,uLatent,vLatent,UV,Test)\n",
    "        #print(\"loss\",loss)\n",
    "        totLoss.append(loss)\n",
    "    return (np.mean(totLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha =  0.1\n",
      "loss =  5.063878816619068\n",
      "Alpha =  0.2\n",
      "loss =  5.03456525902943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "alphaGrid = [0.1,0.2]\n",
    "for each,loss in zip(alphaGrid,bongu):\n",
    "    check = []\n",
    "    P = 10\n",
    "    print(\"Alpha = \", each)\n",
    "    loss = crossValidation(each,Train)\n",
    "    print(\"loss = \",loss)\n",
    "    check.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that for alpha = 0.2 the loss is less, so this parameter is selected, executing the previous statements the train and test loss are\n",
    "\n",
    "Train RMSE = 4.4563827615429825\n",
    "Test RMSE = 3.7691529816539208"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
