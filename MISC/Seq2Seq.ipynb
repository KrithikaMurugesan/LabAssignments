{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kritz/anaconda3/envs/scripts/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 180\n",
      "Number of unique input tokens: 53\n",
      "Number of unique output tokens: 54\n",
      "Max sequence length for inputs: 94\n",
      "Max sequence length for outputs: 111\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.8954 - val_loss: 1.3072\n",
      "Epoch 2/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.7980 - val_loss: 1.3044\n",
      "Epoch 3/400\n",
      "144/144 [==============================] - 9s 61ms/step - loss: 0.7849 - val_loss: 1.2871\n",
      "Epoch 4/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.7775 - val_loss: 1.2864\n",
      "Epoch 5/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.7668 - val_loss: 1.2742\n",
      "Epoch 6/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.7663 - val_loss: 1.2674\n",
      "Epoch 7/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.7538 - val_loss: 1.2525\n",
      "Epoch 8/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.7355 - val_loss: 1.2513\n",
      "Epoch 9/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.7220 - val_loss: 1.2066\n",
      "Epoch 10/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.7033 - val_loss: 1.2062\n",
      "Epoch 11/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.6827 - val_loss: 1.1647\n",
      "Epoch 12/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.6646 - val_loss: 1.1494\n",
      "Epoch 13/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.6498 - val_loss: 1.1153\n",
      "Epoch 14/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.6312 - val_loss: 1.1118\n",
      "Epoch 15/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.6159 - val_loss: 1.1002\n",
      "Epoch 16/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.6043 - val_loss: 1.0533\n",
      "Epoch 17/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.5874 - val_loss: 1.0300\n",
      "Epoch 18/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.5763 - val_loss: 1.0311\n",
      "Epoch 19/400\n",
      "144/144 [==============================] - 9s 61ms/step - loss: 0.5668 - val_loss: 1.0087\n",
      "Epoch 20/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.5543 - val_loss: 1.0401\n",
      "Epoch 21/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.5416 - val_loss: 1.0087\n",
      "Epoch 22/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.5319 - val_loss: 0.9940\n",
      "Epoch 23/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.5200 - val_loss: 0.9818\n",
      "Epoch 24/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.5157 - val_loss: 0.9599\n",
      "Epoch 25/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.5061 - val_loss: 1.1734\n",
      "Epoch 26/400\n",
      "144/144 [==============================] - 10s 66ms/step - loss: 0.5199 - val_loss: 0.9462\n",
      "Epoch 27/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.4888 - val_loss: 0.9556\n",
      "Epoch 28/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.4846 - val_loss: 0.9428\n",
      "Epoch 29/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.4734 - val_loss: 0.9672\n",
      "Epoch 30/400\n",
      "144/144 [==============================] - 11s 74ms/step - loss: 0.4675 - val_loss: 0.9331\n",
      "Epoch 31/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.4598 - val_loss: 0.9362\n",
      "Epoch 32/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.4507 - val_loss: 0.9209\n",
      "Epoch 33/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.4465 - val_loss: 0.9186\n",
      "Epoch 34/400\n",
      "144/144 [==============================] - 12s 86ms/step - loss: 0.4359 - val_loss: 0.9185\n",
      "Epoch 35/400\n",
      "144/144 [==============================] - 12s 87ms/step - loss: 0.4330 - val_loss: 0.9208\n",
      "Epoch 36/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.4265 - val_loss: 0.9170\n",
      "Epoch 37/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.4150 - val_loss: 0.9011\n",
      "Epoch 38/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.4095 - val_loss: 0.9198\n",
      "Epoch 39/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.4019 - val_loss: 0.9019\n",
      "Epoch 40/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.3980 - val_loss: 0.9048\n",
      "Epoch 41/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.3901 - val_loss: 0.9032\n",
      "Epoch 42/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3821 - val_loss: 0.9291\n",
      "Epoch 43/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3754 - val_loss: 0.9285\n",
      "Epoch 44/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.3706 - val_loss: 0.9205\n",
      "Epoch 45/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.3618 - val_loss: 0.9212\n",
      "Epoch 46/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.3534 - val_loss: 0.9470\n",
      "Epoch 47/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.3507 - val_loss: 0.9411\n",
      "Epoch 48/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.3410 - val_loss: 0.9387\n",
      "Epoch 49/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3338 - val_loss: 0.9482\n",
      "Epoch 50/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3289 - val_loss: 0.9584\n",
      "Epoch 51/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3217 - val_loss: 0.9648\n",
      "Epoch 52/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.3135 - val_loss: 0.9534\n",
      "Epoch 53/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.3091 - val_loss: 0.9839\n",
      "Epoch 54/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.3019 - val_loss: 0.9641\n",
      "Epoch 55/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.2943 - val_loss: 0.9830\n",
      "Epoch 56/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.2882 - val_loss: 0.9763\n",
      "Epoch 57/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.2813 - val_loss: 1.0024\n",
      "Epoch 58/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.2755 - val_loss: 0.9952\n",
      "Epoch 59/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.2732 - val_loss: 1.0106\n",
      "Epoch 60/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.2588 - val_loss: 0.9881\n",
      "Epoch 61/400\n",
      "144/144 [==============================] - 13s 90ms/step - loss: 0.2550 - val_loss: 1.0215\n",
      "Epoch 62/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.2498 - val_loss: 1.0309\n",
      "Epoch 63/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.2431 - val_loss: 1.0097\n",
      "Epoch 64/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.2397 - val_loss: 1.0353\n",
      "Epoch 65/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.2306 - val_loss: 1.0695\n",
      "Epoch 66/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.2268 - val_loss: 1.0621\n",
      "Epoch 67/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.2199 - val_loss: 1.0766\n",
      "Epoch 68/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.2144 - val_loss: 1.0729\n",
      "Epoch 69/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.2048 - val_loss: 1.0794\n",
      "Epoch 70/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.2031 - val_loss: 1.0856\n",
      "Epoch 71/400\n",
      "144/144 [==============================] - 11s 73ms/step - loss: 0.1984 - val_loss: 1.1342\n",
      "Epoch 72/400\n",
      "144/144 [==============================] - 11s 74ms/step - loss: 0.1939 - val_loss: 1.1057\n",
      "Epoch 73/400\n",
      "144/144 [==============================] - 11s 75ms/step - loss: 0.1897 - val_loss: 1.1125\n",
      "Epoch 74/400\n",
      "144/144 [==============================] - 11s 77ms/step - loss: 0.1845 - val_loss: 1.1365\n",
      "Epoch 75/400\n",
      "144/144 [==============================] - 11s 75ms/step - loss: 0.1786 - val_loss: 1.1224\n",
      "Epoch 76/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.1729 - val_loss: 1.1401\n",
      "Epoch 77/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.1717 - val_loss: 1.1193\n",
      "Epoch 78/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 11s 74ms/step - loss: 0.1621 - val_loss: 1.1500\n",
      "Epoch 79/400\n",
      "144/144 [==============================] - 11s 74ms/step - loss: 0.1625 - val_loss: 1.1508\n",
      "Epoch 80/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.1534 - val_loss: 1.1712\n",
      "Epoch 81/400\n",
      "144/144 [==============================] - 11s 75ms/step - loss: 0.1539 - val_loss: 1.1581\n",
      "Epoch 82/400\n",
      "144/144 [==============================] - 11s 74ms/step - loss: 0.1471 - val_loss: 1.1816\n",
      "Epoch 83/400\n",
      "144/144 [==============================] - 12s 85ms/step - loss: 0.1450 - val_loss: 1.2122\n",
      "Epoch 84/400\n",
      "144/144 [==============================] - 14s 98ms/step - loss: 0.1411 - val_loss: 1.1888\n",
      "Epoch 85/400\n",
      "144/144 [==============================] - 13s 90ms/step - loss: 0.1364 - val_loss: 1.2133\n",
      "Epoch 86/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.1303 - val_loss: 1.2293\n",
      "Epoch 87/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.1324 - val_loss: 1.2266\n",
      "Epoch 88/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.1267 - val_loss: 1.2285\n",
      "Epoch 89/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.1269 - val_loss: 1.2465\n",
      "Epoch 90/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.1175 - val_loss: 1.2537\n",
      "Epoch 91/400\n",
      "144/144 [==============================] - 12s 83ms/step - loss: 0.1216 - val_loss: 1.2616\n",
      "Epoch 92/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.1114 - val_loss: 1.2804\n",
      "Epoch 93/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.1157 - val_loss: 1.2605\n",
      "Epoch 94/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.1103 - val_loss: 1.2678\n",
      "Epoch 95/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.1087 - val_loss: 1.3098\n",
      "Epoch 96/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.1052 - val_loss: 1.2827\n",
      "Epoch 97/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.1025 - val_loss: 1.2977\n",
      "Epoch 98/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.1015 - val_loss: 1.3273\n",
      "Epoch 99/400\n",
      "144/144 [==============================] - 12s 83ms/step - loss: 0.0984 - val_loss: 1.3364\n",
      "Epoch 100/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.0978 - val_loss: 1.3041\n",
      "Epoch 101/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.0931 - val_loss: 1.3283\n",
      "Epoch 102/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.0999 - val_loss: 1.3341\n",
      "Epoch 103/400\n",
      "144/144 [==============================] - 12s 81ms/step - loss: 0.0896 - val_loss: 1.3578\n",
      "Epoch 104/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.0866 - val_loss: 1.3508\n",
      "Epoch 105/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.0908 - val_loss: 1.3685\n",
      "Epoch 106/400\n",
      "144/144 [==============================] - 13s 88ms/step - loss: 0.0855 - val_loss: 1.3869\n",
      "Epoch 107/400\n",
      "144/144 [==============================] - 14s 95ms/step - loss: 0.0882 - val_loss: 1.3693\n",
      "Epoch 108/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.0832 - val_loss: 1.4152\n",
      "Epoch 109/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.0824 - val_loss: 1.3822\n",
      "Epoch 110/400\n",
      "144/144 [==============================] - 11s 79ms/step - loss: 0.0808 - val_loss: 1.4102\n",
      "Epoch 111/400\n",
      "144/144 [==============================] - 11s 80ms/step - loss: 0.0802 - val_loss: 1.4104\n",
      "Epoch 112/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.3731 - val_loss: 1.2660\n",
      "Epoch 113/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.2664 - val_loss: 1.2987\n",
      "Epoch 114/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.1778 - val_loss: 1.3207\n",
      "Epoch 115/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.1401 - val_loss: 1.2724\n",
      "Epoch 116/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.1346 - val_loss: 1.3009\n",
      "Epoch 117/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.1222 - val_loss: 1.3277\n",
      "Epoch 118/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.1028 - val_loss: 1.3265\n",
      "Epoch 119/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0944 - val_loss: 1.3645\n",
      "Epoch 120/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.0898 - val_loss: 1.3418\n",
      "Epoch 121/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0840 - val_loss: 1.3511\n",
      "Epoch 122/400\n",
      "144/144 [==============================] - 10s 73ms/step - loss: 0.0876 - val_loss: 1.3504\n",
      "Epoch 123/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0821 - val_loss: 1.3847\n",
      "Epoch 124/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0799 - val_loss: 1.3855\n",
      "Epoch 125/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0789 - val_loss: 1.3790\n",
      "Epoch 126/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0785 - val_loss: 1.4192\n",
      "Epoch 127/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0767 - val_loss: 1.4256\n",
      "Epoch 128/400\n",
      "144/144 [==============================] - 9s 66ms/step - loss: 0.0741 - val_loss: 1.3972\n",
      "Epoch 129/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0754 - val_loss: 1.4056\n",
      "Epoch 130/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0724 - val_loss: 1.4456\n",
      "Epoch 131/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0725 - val_loss: 1.4246\n",
      "Epoch 132/400\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.0702 - val_loss: 1.4525\n",
      "Epoch 133/400\n",
      "144/144 [==============================] - 6s 39ms/step - loss: 0.0702 - val_loss: 1.4521\n",
      "Epoch 134/400\n",
      "144/144 [==============================] - 9s 60ms/step - loss: 0.0692 - val_loss: 1.4871\n",
      "Epoch 135/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0706 - val_loss: 1.4814\n",
      "Epoch 136/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0664 - val_loss: 1.4756\n",
      "Epoch 137/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0686 - val_loss: 1.4817\n",
      "Epoch 138/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.0652 - val_loss: 1.4667\n",
      "Epoch 139/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0672 - val_loss: 1.4535\n",
      "Epoch 140/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0644 - val_loss: 1.5057\n",
      "Epoch 141/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0650 - val_loss: 1.4921\n",
      "Epoch 142/400\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 0.0645 - val_loss: 1.5097\n",
      "Epoch 143/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.0625 - val_loss: 1.5123\n",
      "Epoch 144/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0627 - val_loss: 1.5034\n",
      "Epoch 145/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0620 - val_loss: 1.5014\n",
      "Epoch 146/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0611 - val_loss: 1.5365\n",
      "Epoch 147/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.0608 - val_loss: 1.4868\n",
      "Epoch 148/400\n",
      "144/144 [==============================] - 12s 84ms/step - loss: 0.0601 - val_loss: 1.5521\n",
      "Epoch 149/400\n",
      "144/144 [==============================] - 12s 80ms/step - loss: 0.0612 - val_loss: 1.5294\n",
      "Epoch 150/400\n",
      "144/144 [==============================] - 11s 78ms/step - loss: 0.0586 - val_loss: 1.5340\n",
      "Epoch 151/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.0600 - val_loss: 1.5413\n",
      "Epoch 152/400\n",
      "144/144 [==============================] - 12s 86ms/step - loss: 0.0602 - val_loss: 1.5574\n",
      "Epoch 153/400\n",
      "144/144 [==============================] - 11s 75ms/step - loss: 0.0598 - val_loss: 1.5381\n",
      "Epoch 154/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0587 - val_loss: 1.5416\n",
      "Epoch 155/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0574 - val_loss: 1.5319\n",
      "Epoch 156/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 10s 69ms/step - loss: 0.0572 - val_loss: 1.5585\n",
      "Epoch 157/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0569 - val_loss: 1.5596\n",
      "Epoch 158/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.0561 - val_loss: 1.5682\n",
      "Epoch 159/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0570 - val_loss: 1.5654\n",
      "Epoch 160/400\n",
      "144/144 [==============================] - 9s 60ms/step - loss: 0.0554 - val_loss: 1.5769\n",
      "Epoch 161/400\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.0553 - val_loss: 1.5759\n",
      "Epoch 162/400\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0547 - val_loss: 1.5751\n",
      "Epoch 163/400\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0550 - val_loss: 1.6132\n",
      "Epoch 164/400\n",
      "144/144 [==============================] - 6s 42ms/step - loss: 0.0548 - val_loss: 1.5863\n",
      "Epoch 165/400\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.0727 - val_loss: 1.5679\n",
      "Epoch 166/400\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0545 - val_loss: 1.5751\n",
      "Epoch 167/400\n",
      "144/144 [==============================] - 6s 41ms/step - loss: 0.0511 - val_loss: 1.6201\n",
      "Epoch 168/400\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.0549 - val_loss: 1.5934\n",
      "Epoch 169/400\n",
      "144/144 [==============================] - 9s 61ms/step - loss: 0.0535 - val_loss: 1.5992\n",
      "Epoch 170/400\n",
      "144/144 [==============================] - 11s 75ms/step - loss: 0.0550 - val_loss: 1.5852\n",
      "Epoch 171/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0571 - val_loss: 1.6301\n",
      "Epoch 172/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0550 - val_loss: 1.6178\n",
      "Epoch 173/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0535 - val_loss: 1.6169\n",
      "Epoch 174/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0525 - val_loss: 1.6044\n",
      "Epoch 175/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0506 - val_loss: 1.5971\n",
      "Epoch 176/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0511 - val_loss: 1.6242\n",
      "Epoch 177/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0517 - val_loss: 1.6586\n",
      "Epoch 178/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0507 - val_loss: 1.6277\n",
      "Epoch 179/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0508 - val_loss: 1.6601\n",
      "Epoch 180/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0500 - val_loss: 1.6563\n",
      "Epoch 181/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0496 - val_loss: 1.6277\n",
      "Epoch 182/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0486 - val_loss: 1.6198\n",
      "Epoch 183/400\n",
      "144/144 [==============================] - 10s 73ms/step - loss: 0.0509 - val_loss: 1.6703\n",
      "Epoch 184/400\n",
      "144/144 [==============================] - 11s 73ms/step - loss: 0.0496 - val_loss: 1.6502\n",
      "Epoch 185/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.0484 - val_loss: 1.6675\n",
      "Epoch 186/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.0477 - val_loss: 1.6658\n",
      "Epoch 187/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0491 - val_loss: 1.6592\n",
      "Epoch 188/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0477 - val_loss: 1.6635\n",
      "Epoch 189/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0492 - val_loss: 1.6936\n",
      "Epoch 190/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0468 - val_loss: 1.6707\n",
      "Epoch 191/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0469 - val_loss: 1.6682\n",
      "Epoch 192/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0479 - val_loss: 1.6582\n",
      "Epoch 193/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0489 - val_loss: 1.6755\n",
      "Epoch 194/400\n",
      "144/144 [==============================] - 11s 73ms/step - loss: 0.0470 - val_loss: 1.6875\n",
      "Epoch 195/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0459 - val_loss: 1.6900\n",
      "Epoch 196/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.0460 - val_loss: 1.6813\n",
      "Epoch 197/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0472 - val_loss: 1.7103\n",
      "Epoch 198/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0456 - val_loss: 1.7026\n",
      "Epoch 199/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0461 - val_loss: 1.6904\n",
      "Epoch 200/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0452 - val_loss: 1.6891\n",
      "Epoch 201/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0446 - val_loss: 1.7057\n",
      "Epoch 202/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0449 - val_loss: 1.6859\n",
      "Epoch 203/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0446 - val_loss: 1.7049\n",
      "Epoch 204/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0450 - val_loss: 1.6979\n",
      "Epoch 205/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0453 - val_loss: 1.7112\n",
      "Epoch 206/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0450 - val_loss: 1.7175\n",
      "Epoch 207/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0437 - val_loss: 1.6906\n",
      "Epoch 208/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0607 - val_loss: 1.7055\n",
      "Epoch 209/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0436 - val_loss: 1.7005\n",
      "Epoch 210/400\n",
      "144/144 [==============================] - 10s 71ms/step - loss: 0.0414 - val_loss: 1.6989\n",
      "Epoch 211/400\n",
      "144/144 [==============================] - 12s 82ms/step - loss: 0.0414 - val_loss: 1.7235\n",
      "Epoch 212/400\n",
      "144/144 [==============================] - 11s 76ms/step - loss: 0.0431 - val_loss: 1.7193\n",
      "Epoch 213/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0433 - val_loss: 1.7029\n",
      "Epoch 214/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0419 - val_loss: 1.7187\n",
      "Epoch 215/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0421 - val_loss: 1.7152\n",
      "Epoch 216/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0500 - val_loss: 1.6773\n",
      "Epoch 217/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0447 - val_loss: 1.6922\n",
      "Epoch 218/400\n",
      "144/144 [==============================] - 10s 66ms/step - loss: 0.0413 - val_loss: 1.7031\n",
      "Epoch 219/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0403 - val_loss: 1.7071\n",
      "Epoch 220/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0421 - val_loss: 1.7196\n",
      "Epoch 221/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0430 - val_loss: 1.6991\n",
      "Epoch 222/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0418 - val_loss: 1.7142\n",
      "Epoch 223/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0419 - val_loss: 1.7496\n",
      "Epoch 224/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0433 - val_loss: 1.7484\n",
      "Epoch 225/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0428 - val_loss: 1.7296\n",
      "Epoch 226/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0421 - val_loss: 1.7392\n",
      "Epoch 227/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0412 - val_loss: 1.7515\n",
      "Epoch 228/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0429 - val_loss: 1.7513\n",
      "Epoch 229/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0411 - val_loss: 1.7481\n",
      "Epoch 230/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0405 - val_loss: 1.7326\n",
      "Epoch 231/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0413 - val_loss: 1.7556\n",
      "Epoch 232/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0420 - val_loss: 1.7836\n",
      "Epoch 233/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0542 - val_loss: 1.7079\n",
      "Epoch 234/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0380 - val_loss: 1.7567\n",
      "Epoch 235/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0392 - val_loss: 1.7219\n",
      "Epoch 236/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0393 - val_loss: 1.7200\n",
      "Epoch 237/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0398 - val_loss: 1.7022\n",
      "Epoch 238/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0400 - val_loss: 1.7318\n",
      "Epoch 239/400\n",
      "144/144 [==============================] - 9s 61ms/step - loss: 0.0405 - val_loss: 1.7400\n",
      "Epoch 240/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0402 - val_loss: 1.7102\n",
      "Epoch 241/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0403 - val_loss: 1.7260\n",
      "Epoch 242/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0398 - val_loss: 1.7059\n",
      "Epoch 243/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0391 - val_loss: 1.7032\n",
      "Epoch 244/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0378 - val_loss: 1.7054\n",
      "Epoch 245/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0391 - val_loss: 1.7202\n",
      "Epoch 246/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0390 - val_loss: 1.7082\n",
      "Epoch 247/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0389 - val_loss: 1.7491\n",
      "Epoch 248/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0377 - val_loss: 1.7380\n",
      "Epoch 249/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0409 - val_loss: 1.7444\n",
      "Epoch 250/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0376 - val_loss: 1.7292\n",
      "Epoch 251/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0378 - val_loss: 1.7523\n",
      "Epoch 252/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0398 - val_loss: 1.7455\n",
      "Epoch 253/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0376 - val_loss: 1.7502\n",
      "Epoch 254/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0384 - val_loss: 1.7683\n",
      "Epoch 255/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0385 - val_loss: 1.7589\n",
      "Epoch 256/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0381 - val_loss: 1.7858\n",
      "Epoch 257/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0388 - val_loss: 1.7682\n",
      "Epoch 258/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0383 - val_loss: 1.7638\n",
      "Epoch 259/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0375 - val_loss: 1.7657\n",
      "Epoch 260/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0382 - val_loss: 1.7485\n",
      "Epoch 261/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0379 - val_loss: 1.7573\n",
      "Epoch 262/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0381 - val_loss: 1.7959\n",
      "Epoch 263/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0369 - val_loss: 1.7970\n",
      "Epoch 264/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0351 - val_loss: 1.7782\n",
      "Epoch 265/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0376 - val_loss: 1.7884\n",
      "Epoch 266/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0361 - val_loss: 1.7860\n",
      "Epoch 267/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0374 - val_loss: 1.8014\n",
      "Epoch 268/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0375 - val_loss: 1.7950\n",
      "Epoch 269/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0358 - val_loss: 1.7920\n",
      "Epoch 270/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0364 - val_loss: 1.7949\n",
      "Epoch 271/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0350 - val_loss: 1.8085\n",
      "Epoch 272/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0370 - val_loss: 1.8150\n",
      "Epoch 273/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0363 - val_loss: 1.8127\n",
      "Epoch 274/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0340 - val_loss: 1.7927\n",
      "Epoch 275/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0365 - val_loss: 1.8057\n",
      "Epoch 276/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0361 - val_loss: 1.8035\n",
      "Epoch 277/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0350 - val_loss: 1.8306\n",
      "Epoch 278/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0498 - val_loss: 1.8070\n",
      "Epoch 279/400\n",
      "144/144 [==============================] - 10s 69ms/step - loss: 0.0368 - val_loss: 1.7500\n",
      "Epoch 280/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0363 - val_loss: 1.8089\n",
      "Epoch 281/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0383 - val_loss: 1.7848\n",
      "Epoch 282/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0341 - val_loss: 1.7389\n",
      "Epoch 283/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0334 - val_loss: 1.7389\n",
      "Epoch 284/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0328 - val_loss: 1.7777\n",
      "Epoch 285/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0332 - val_loss: 1.7590\n",
      "Epoch 286/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0357 - val_loss: 1.7533\n",
      "Epoch 287/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0345 - val_loss: 1.7741\n",
      "Epoch 288/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0334 - val_loss: 1.7969\n",
      "Epoch 289/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0345 - val_loss: 1.7806\n",
      "Epoch 290/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0338 - val_loss: 1.7955\n",
      "Epoch 291/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0346 - val_loss: 1.8221\n",
      "Epoch 292/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0337 - val_loss: 1.7915\n",
      "Epoch 293/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0362 - val_loss: 1.8096\n",
      "Epoch 294/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0324 - val_loss: 1.8105\n",
      "Epoch 295/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0444 - val_loss: 1.7820\n",
      "Epoch 296/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0667 - val_loss: 1.7710\n",
      "Epoch 297/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0358 - val_loss: 1.7905\n",
      "Epoch 298/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0314 - val_loss: 1.7914\n",
      "Epoch 299/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0333 - val_loss: 1.8314\n",
      "Epoch 300/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0323 - val_loss: 1.8352\n",
      "Epoch 301/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0334 - val_loss: 1.8215\n",
      "Epoch 302/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0327 - val_loss: 1.8204\n",
      "Epoch 303/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0345 - val_loss: 1.8185\n",
      "Epoch 304/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0330 - val_loss: 1.7838\n",
      "Epoch 305/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0363 - val_loss: 1.8276\n",
      "Epoch 306/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0330 - val_loss: 1.8290\n",
      "Epoch 307/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0305 - val_loss: 1.8524\n",
      "Epoch 308/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0323 - val_loss: 1.8490\n",
      "Epoch 309/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0351 - val_loss: 1.8512\n",
      "Epoch 310/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0308 - val_loss: 1.8407\n",
      "Epoch 311/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0312 - val_loss: 1.8574\n",
      "Epoch 312/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0307 - val_loss: 1.8375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0313 - val_loss: 1.8559\n",
      "Epoch 314/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0343 - val_loss: 1.8608\n",
      "Epoch 315/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0335 - val_loss: 1.8615\n",
      "Epoch 316/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0297 - val_loss: 1.8364\n",
      "Epoch 317/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0314 - val_loss: 1.8585\n",
      "Epoch 318/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0302 - val_loss: 1.8687\n",
      "Epoch 319/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0322 - val_loss: 1.9020\n",
      "Epoch 320/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0310 - val_loss: 1.8516\n",
      "Epoch 321/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0335 - val_loss: 1.8821\n",
      "Epoch 322/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0314 - val_loss: 1.8935\n",
      "Epoch 323/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0314 - val_loss: 1.8804\n",
      "Epoch 324/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0306 - val_loss: 1.8702\n",
      "Epoch 325/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0304 - val_loss: 1.8724\n",
      "Epoch 326/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0311 - val_loss: 1.8952\n",
      "Epoch 327/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0316 - val_loss: 1.8906\n",
      "Epoch 328/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0316 - val_loss: 1.8895\n",
      "Epoch 329/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0316 - val_loss: 1.8599\n",
      "Epoch 330/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0305 - val_loss: 1.8745\n",
      "Epoch 331/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0303 - val_loss: 1.9106\n",
      "Epoch 332/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0312 - val_loss: 1.9120\n",
      "Epoch 333/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0290 - val_loss: 1.8884\n",
      "Epoch 334/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0314 - val_loss: 1.9214\n",
      "Epoch 335/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0326 - val_loss: 1.8552\n",
      "Epoch 336/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0323 - val_loss: 1.8410\n",
      "Epoch 337/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0350 - val_loss: 1.8558\n",
      "Epoch 338/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0277 - val_loss: 1.8639\n",
      "Epoch 339/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0300 - val_loss: 1.8853\n",
      "Epoch 340/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0278 - val_loss: 1.8768\n",
      "Epoch 341/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0287 - val_loss: 1.8768\n",
      "Epoch 342/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0271 - val_loss: 1.8766\n",
      "Epoch 343/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0293 - val_loss: 1.8659\n",
      "Epoch 344/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0295 - val_loss: 1.8661\n",
      "Epoch 345/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0290 - val_loss: 1.8600\n",
      "Epoch 346/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0286 - val_loss: 1.8760\n",
      "Epoch 347/400\n",
      "144/144 [==============================] - 9s 65ms/step - loss: 0.0297 - val_loss: 1.8936\n",
      "Epoch 348/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0285 - val_loss: 1.9214\n",
      "Epoch 349/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0281 - val_loss: 1.8441\n",
      "Epoch 350/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0292 - val_loss: 1.8846\n",
      "Epoch 351/400\n",
      "144/144 [==============================] - 10s 70ms/step - loss: 0.0280 - val_loss: 1.8852\n",
      "Epoch 352/400\n",
      "144/144 [==============================] - 10s 72ms/step - loss: 0.0267 - val_loss: 1.8854\n",
      "Epoch 353/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0300 - val_loss: 1.9173\n",
      "Epoch 354/400\n",
      "144/144 [==============================] - 10s 67ms/step - loss: 0.0295 - val_loss: 1.8531\n",
      "Epoch 355/400\n",
      "144/144 [==============================] - 9s 61ms/step - loss: 0.0334 - val_loss: 1.8593\n",
      "Epoch 356/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0292 - val_loss: 1.8782\n",
      "Epoch 357/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0282 - val_loss: 1.8921\n",
      "Epoch 358/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0260 - val_loss: 1.9303\n",
      "Epoch 359/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0265 - val_loss: 1.8926\n",
      "Epoch 360/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0276 - val_loss: 1.8845\n",
      "Epoch 361/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0287 - val_loss: 1.8926\n",
      "Epoch 362/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0303 - val_loss: 1.8870\n",
      "Epoch 363/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0309 - val_loss: 1.9208\n",
      "Epoch 364/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0309 - val_loss: 1.8806\n",
      "Epoch 365/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0267 - val_loss: 1.9485\n",
      "Epoch 366/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0305 - val_loss: 1.8962\n",
      "Epoch 367/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0291 - val_loss: 1.8875\n",
      "Epoch 368/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0303 - val_loss: 1.9151\n",
      "Epoch 369/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0272 - val_loss: 1.8954\n",
      "Epoch 370/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0272 - val_loss: 1.9055\n",
      "Epoch 371/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0269 - val_loss: 1.8740\n",
      "Epoch 372/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0253 - val_loss: 1.8613\n",
      "Epoch 373/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0272 - val_loss: 1.9211\n",
      "Epoch 374/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0289 - val_loss: 1.8949\n",
      "Epoch 375/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0264 - val_loss: 1.9202\n",
      "Epoch 376/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0289 - val_loss: 1.9218\n",
      "Epoch 377/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0268 - val_loss: 1.9161\n",
      "Epoch 378/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0273 - val_loss: 1.9309\n",
      "Epoch 379/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0279 - val_loss: 1.9499\n",
      "Epoch 380/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0283 - val_loss: 1.9291\n",
      "Epoch 381/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0259 - val_loss: 1.9283\n",
      "Epoch 382/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0253 - val_loss: 1.9354\n",
      "Epoch 383/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0273 - val_loss: 1.9052\n",
      "Epoch 384/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0272 - val_loss: 1.9164\n",
      "Epoch 385/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0251 - val_loss: 1.9388\n",
      "Epoch 386/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0268 - val_loss: 1.9435\n",
      "Epoch 387/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0278 - val_loss: 1.9446\n",
      "Epoch 388/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0264 - val_loss: 1.9147\n",
      "Epoch 389/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0271 - val_loss: 1.9172\n",
      "Epoch 390/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0255 - val_loss: 1.9139\n",
      "Epoch 391/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0278 - val_loss: 1.9119\n",
      "Epoch 392/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0253 - val_loss: 1.9380\n",
      "Epoch 393/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0257 - val_loss: 1.9166\n",
      "Epoch 394/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0264 - val_loss: 1.9315\n",
      "Epoch 395/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0247 - val_loss: 1.9192\n",
      "Epoch 396/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0267 - val_loss: 1.9441\n",
      "Epoch 397/400\n",
      "144/144 [==============================] - 9s 62ms/step - loss: 0.0255 - val_loss: 1.9456\n",
      "Epoch 398/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0274 - val_loss: 1.9425\n",
      "Epoch 399/400\n",
      "144/144 [==============================] - 9s 64ms/step - loss: 0.0255 - val_loss: 1.9276\n",
      "Epoch 400/400\n",
      "144/144 [==============================] - 9s 63ms/step - loss: 0.0236 - val_loss: 1.9935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kritz/anaconda3/envs/scripts/lib/python3.6/site-packages/keras/engine/topology.py:2368: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Calm down.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Who is he?\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Who knows?\n",
      "Decoded sentence:  ?\n",
      "\n",
      "-\n",
      "Input sentence: She smiled.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Talk to me!\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Who is she?\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Go to sleep.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: It may rain.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: She bit him.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: She hit him.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: She is kind.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: She is eight.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Where are we?\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Keep in touch!\n",
      "Decoded sentence:    ?\n",
      "\n",
      "-\n",
      "Input sentence: See you again.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Give it to her.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: I ate too much.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I'll see to it.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: It's up to you.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Leave it to me.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: Listen to this!\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: That's the way.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Come and see me.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Don't lie to me.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: He began to run.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: He just arrived.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: He likes to run.\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: How is your dad?\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I want to sleep.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I'm able to run.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: Raise your hand.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: What did he say?\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: When can we eat?\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: Come and help us.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: He is still here.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: I have to go now.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: I made a mistake.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I walk to school.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: That's our house.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Those are my CDs.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Walk ahead of me.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: Beware of the dog!\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: He came back soon.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: He has three sons.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I know how to ski.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I know what to do.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I'm kind of happy.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: Keep to the right.\n",
      "Decoded sentence:    ?\n",
      "\n",
      "-\n",
      "Input sentence: She began to sing.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: She decided to go.\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: Do I have to study?\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: He is sure to come.\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: I had to walk home.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I have to dress up.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: I told him to come.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I'm short of money.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: May I speak to you?\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: She gave it to him.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: She is kind to him.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: She sat next to me.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Shut up and listen!\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Tell me what to do.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Tom runs very fast.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: We ran out of food.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: We started to walk.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: When does it begin?\n",
      "Decoded sentence:  ?\n",
      "\n",
      "-\n",
      "Input sentence: Are you ready to go?\n",
      "Decoded sentence:    ?\n",
      "\n",
      "-\n",
      "Input sentence: Do you have any gum?\n",
      "Decoded sentence:    ?\n",
      "\n",
      "-\n",
      "Input sentence: Does she play piano?\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: Don't listen to her.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Go and wake Mary up.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: He seems to know us.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I am engaged to her.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I have to leave now.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: I want to go abroad.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I'm glad to see you.\n",
      "Decoded sentence:      \n",
      "\n",
      "-\n",
      "Input sentence: I'm proud of my son.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: I'm taller than you.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: I'm trying to sleep.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: It's free of charge.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: It's time to get up.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Nobody speaks to me.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Roll the ball to me.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: She boiled the eggs.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: She danced with him.\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: She gave him a book.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: She has 2,000 books.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: This apple is sweet.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: We swam in the lake.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Come home before six.\n",
      "Decoded sentence:    \n",
      "\n",
      "-\n",
      "Input sentence: Go and see who it is.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: I am afraid of bears.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: I expect him to come.\n",
      "Decoded sentence:     \n",
      "\n",
      "-\n",
      "Input sentence: It's a piece of cake.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: The boy began to cry.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: You keep out of this.\n",
      "Decoded sentence:   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: All of us were silent.\n",
      "Decoded sentence:   ?\n",
      "\n",
      "-\n",
      "Input sentence: Be kind to old people.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Beware of pickpockets.\n",
      "Decoded sentence:   \n",
      "\n",
      "-\n",
      "Input sentence: Don't drink and drive.\n",
      "Decoded sentence:   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size =15  # Batch size for training.\n",
    "epochs = 400  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'tam.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Talk to me!\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: She is kind.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: Give it to her.\n",
      "Decoded sentence:  \n",
      "\n",
      "-\n",
      "Input sentence: That's the way.\n",
      "Decoded sentence:   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_index = [4,10,15,21]\n",
    "for each in seq_index:\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[each: each + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[each])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
